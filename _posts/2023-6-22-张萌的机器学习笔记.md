---
layout: single
title: 张萌的机器学习笔记
date: 2023-06-22
excerpt: 学习机器学习
categories:
- Machine Learning
tags:
- Machine Learning
---

{% include toc title = "目录" %}
# 机器学习（Machine Learning）

## 什么是机器学习？

· 机器学习属于人工智能领域。

· 顾名思义，机器学习即让机器来学习解决问题。

· 机器通过执行任务T，获得经验E以此提高度量P。

### 一些专业术语

监督学习：我们教会计算机某件事情。

无监督学习：我们让计算机自己学习。

（以上为最常用的两种学习算法）

强化学习：

推荐系统：

## 第一章：监督学习与无监督学习

### 监督学习：supervised learning

#### 回归问题：regression problem

目的：预测连续值的输出（实数、数量）

#### 分类问题：classification problem

目的：预测离散值输出（选择、分类）

### 无监督学习：unsupervised learning

#### 区别

在监督学习中，我们给机器带有标签的数据库

每个样本都被清楚标记了良性/恶性，即知晓正确答案。

但在无监督学习中，我们的数据库有所不同：所有样本都没有标签或具有相同的标签，我们不知道拿到这些数据应该做什么。

我们只被告知这里有一个数据集，你能在这里找到某种结构吗？

总而言之，**无监督学习就是一种你给算法大量数据，要求它找出算法的数据的类型结构的学习机制。**

#### 聚类算法（clustering algorithm）

可能会使机器如下分类：根据它们聚合程度的判断，将其分成两个蔟。

## 第二章：第一个学习算法

### 线性回归

#### 模型描述

图中h表示假设函数：用于描述x和y之间的关系（关系可能不存在）

### 代价函数：Cost Function

1.均方误差损失函数
2.交叉熵损失函数

#### 假设函数与代价函数

**最小化代价函数：确定一个/多个足够好的参数，使得代价函数尽量小，假设函数足够贴合实际情况。**

##### 一：简化参数

我们将函数h中的θ0设为0，即只观察θ1对假设函数与代价函数的影响。

左侧为假设函数，其固定θ1（斜率），以x为自变量。           	右侧为代价函数，其以θ1为自变量。

不断地假设θ1的值代入到假设函数h中，绘制下图：

根据上图绘制代价函数J
不断假设θ1来绘制下图：

可以观察到，**当θ1=1时，J函数=0。即此时误差平方为零，假设函数与实际值最贴合。**

这就是我们为什么要最小化代价函数。


### 梯度下降算法：Gradient descent

#### 作用

常常用于最小化函数（如代价函数）

 #### 问题概述

最小化假设函数。（可应用于n个参数的假设函数，为了简化问题，只举例两个参数的假设函数）

#### 梯度下降的思路

第一步：给参数设某个初始值（设什么值不重要，一般都初始化为0）

第二步：持续变化参数值来减少假设函数值，直至假设函数到达最小值/极小值。

#### 形象化梯度下降

当你走在一个山上，每次都要走下降最快的方向，直至到达局部最低点：

但一旦你的出发点稍稍偏离，你的终点也可能会跟着改变。

#### 算法实现

```
:=表示赋值。

α表示学习率，决定梯度下降时，我们迈多大的步子，即梯度下降的变化率。（α越大，迈步越大）
同时也控制参数的变化率。

for j = 0 and j = 1  表示同时更新θ0和θ1。
下式为正确同时更新的方法。
```

下图详细介绍了求导数的作用

学习率的作用：调整梯度的变化速率

当已下降到局部最低点时，再执行梯度下降算法会发生什么？

答案是，**不会变，仍然停留在该局部最低点。** **因为局部最低点的斜率/导数为零。**



我们不用改变学习率也能找到局部最低点，为什么？

因为**越接近局部最低点，斜率/导数越小，每次迈的步伐也越小**，不易错过局部最低点。

### 线性回归的梯度下降

用梯度下降算法最小化平方差代价函数。

