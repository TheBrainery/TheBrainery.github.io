# 机器学习（Machine Learning）

## 什么是机器学习？

· 机器学习属于人工智能领域。

· 顾名思义，机器学习即让机器来学习解决问题。

· 机器通过执行任务T，获得经验E以此提高度量P。

### 一些专业术语

监督学习：我们教会计算机某件事情。

无监督学习：我们让计算机自己学习。

（以上为最常用的两种学习算法）

强化学习：

推荐系统：

## 第一章：监督学习与无监督学习

### 监督学习：supervised learning

#### 引例

给机器大量的良性/恶性肿瘤数据库，其中包含两个**特征**：肿瘤大小、良性/恶性。

如下图：

![image-20220118173328678](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220118173328678.png)

其中蓝色表示良性，红色表示恶性。

一个人的肿瘤位于粉色箭头指向的位置，**机器学习需要做的就是判断这个人的肿瘤是良性还是恶性。**

![image-20220118173232330](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220118173232330.png)

#### 回归问题：regression problem

目的：预测连续值的输出（实数、数量）

#### 分类问题：classification problem

目的：预测离散值输出（选择、分类）

### 无监督学习：unsupervised learning

#### 区别

在监督学习中，我们给机器如下图一样的数据库：

![image-20220118173700201](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220118173700201.png)

每个样本都被清楚标记了良性/恶性，即知晓正确答案。

但在无监督学习中，我们的数据库有所不同：

![image-20220118173840856](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220118173840856.png)

所有样本都没有标签或具有相同的标签，我们不知道拿到这些数据应该做什么。

我们只被告知这里有一个数据集，你能在这里找到某种结构吗？

总而言之，**无监督学习就是一种你给算法大量数据，要求它找出算法的数据的类型结构的学习机制。**

#### 聚类算法（clustering algorithm）

可能会使机器如下分类：

![image-20220118174120949](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220118174120949.png)

根据它们聚合程度的判断，将其分成两个蔟。

## 第二章：第一个学习算法

### 线性回归

#### 模型描述

![image-20220118181037724](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220118181037724.png)

![image-20220118180958668](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220118180958668.png)

图中h表示假设函数：用于描述x和y之间的关系（关系可能不存在）

### 代价函数：Cost Function

![image-20220119095443547](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119095443547.png)

（图中函数J为代价函数中的误差平方函数）

#### 假设函数与代价函数

**最小化代价函数：确定一个/多个足够好的参数，使得代价函数尽量小，假设函数足够贴合实际情况。**

##### 一：简化参数

我们将函数h中的θ0设为0，即只观察θ1对假设函数与代价函数的影响。

![image-20220119101106027](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119101106027.png)

左侧为假设函数，其固定θ1（斜率），以x为自变量。           	右侧为代价函数，其以θ1为自变量。

不断地假设θ1的值代入到假设函数h中，绘制下图：

![image-20220119101333256](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119101333256.png)

根据上图绘制代价函数J

![image-20220119101436546](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119101436546.png)

不断假设θ1来绘制下图：

![image-20220119100700752](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119100700752.png)

可以观察到，**当θ1=1时，J函数=0。即此时误差平方为零，假设函数与实际值最贴合。**

这就是我们为什么要最小化代价函数。

##### 二：两个参数

![image-20220119101731966](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119101731966.png)

这一次，我们不再简化参数，两个参数都进行保留。

假设函数如下图：

![image-20220119103021431](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119103021431.png)

不难发现，代价函数中，随着自变量的增加，该函数的图像仍为碗状图，但需要升至三维：

![image-20220119101948849](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119101948849.png)

为了方便研究，我们将这个碗状图投射至θ1和θ0的平面：

![image-20220119102720632](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119102720632.png)

其中这三点（位于同一颜色的线上）的任意参数值，都对应相同的代价函数。

图中椭圆线越小，代价函数越小，对应的假设函数越好。

### 梯度下降算法：Gradient descent

#### 作用

常常用于最小化函数（如代价函数）

 #### 问题概述

![image-20220119103546791](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119103546791.png)



最小化假设函数。（可应用于n个参数的假设函数，为了简化问题，只举例两个参数的假设函数）

#### 梯度下降的思路

![image-20220119103727938](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119103727938.png)

第一步：给参数设某个初始值（设什么值不重要，一般都初始化为0）

第二步：持续变化参数值来减少假设函数值，直至假设函数到达最小值/极小值。

#### 形象化梯度下降

当你走在一个山上，每次都要走下降最快的方向，直至到达局部最低点：

![image-20220119104441664](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119104441664.png)

但一旦你的出发点稍稍偏离，你的终点也可能会跟着改变：

![image-20220119104348550](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119104348550.png)

#### 算法实现



![image-20220119104729292](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119104729292.png)

```
:=表示赋值。

α表示学习率，决定梯度下降时，我们迈多大的步子，即梯度下降的变化率。（α越大，迈步越大）
同时也控制参数的变化率。

for j = 0 and j = 1  表示同时更新θ0和θ1。
下式为正确同时更新的方法。
```

下图详细介绍了求导数的作用：

![image-20220119110508410](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119110508410.png)

学习率的作用：

![image-20220119110845651](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119110845651.png)

当已下降到局部最低点时，再执行梯度下降算法会发生什么？

![image-20220119111104675](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119111104675.png)

答案是，**不会变，仍然停留在该局部最低点。** **因为局部最低点的斜率/导数为零。**



我们不用改变学习率也能找到局部最低点，为什么？

![image-20220119111722516](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119111722516.png)

因为**越接近局部最低点，斜率/导数越小，每次迈的步伐也越小**，不易错过局部最低点。

### 线性回归的梯度下降

用梯度下降算法最小化平方差代价函数。

![image-20220119112118918](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119112118918.png)

![image-20220119113213288](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220119113213288.png)



## 第三章：矩阵

### 矩阵和向量

#### 矩阵的基本概念：Matrix

用方括号[]括住一些数据，用于表示矩阵。

表示某个特定规格的矩阵：

![image-20220120095407335](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120095407335.png)

表示矩阵中的某个元素：

![image-20220120095326327](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120095326327.png)

#### 向量：Vector

向量是特殊的矩阵，只有一列的矩阵。

向量的维数由行数决定，如下图向量y为四维向量。

![image-20220120103505780](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120103505780.png)

表示向量中的某个元素：

![image-20220120103852830](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120103852830.png)

表示向量中的元素有两种方法：1标法和0标法。

数学中常常使用1标法，而机器学习中常常使用0标法。**（在后续课程中，若未特殊备注，皆为1标法。）**

![image-20220120103946492](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120103946492.png)

#### 矩阵和向量的区别

我们常用大写字母表示矩阵，小写字母表示向量。

如下式则表示一个3×4维矩阵：
$$
R^{3×4}
$$
如下式则表示一个三维向量：
$$
R^3
$$

### 加法和标量乘法

#### 矩阵加法：Matrix Addition

矩阵加法即将对应相同位置的数据元素进行相加：

![image-20220120105004127](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120105004127.png)

所以，**只有相同规模/维度的矩阵可以相加，得到的矩阵与其规模/维度相同。**

#### 标量乘法：Scalar Multiplication

在这里，我们的标量指实数。

标量乘法即将实数乘进矩阵中的所有元素。

![image-20220120105436744](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120105436744.png)

由上图，可以发现，**得到的矩阵与相乘的矩阵维度相同，且标量乘法满足交换律。**

#### 混合计算

下图为标量乘法和矩阵加法的混合计算：

![image-20220120105817966](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120105817966.png)

不难发现，最终结果也是一个三维向量。

### 矩阵向量乘法

#### 矩阵与向量乘法步骤

![image-20220120110141364](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120110141364.png)

#### 应用

题中已给自变量（房屋大小）以及对应函数（出售价格）时，要求计算不同房屋大小对应的出售价格。

![image-20220120110726462](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120110726462.png)

这时我们可以使用矩阵向量乘法，来计算房屋价格：

![image-20220120110648516](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120110648516.png)

根据上图可整理出公式：
$$
数据矩阵*参数向量=期待结果向量
$$
数据矩阵存储代入函数中自变量的值，参数向量存储函数中的所有参数。二者相乘的结果得到的向量对应数据即为问题结果。

这一公式常常用于编程语言中，它可以减少代码量，提高效率。

### 矩阵乘法

#### 步骤

![image-20220120111942706](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120111942706.png)

![image-20220120112134065](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120112134065.png)

将矩阵B拆分为o个m维向量，再与矩阵A相乘，转换为矩阵向量乘法。

#### 应用

题中已给自变量（房屋大小）以及**多个**对应函数（出售价格）时，要求计算不同房屋大小对应的出售价格。

这时我们可以使用矩阵乘法解决问题：

![image-20220120112454272](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120112454272.png)

根据上图可整理出公式：
$$
数据矩阵*参数矩阵=期待结果矩阵
$$
数据矩阵存储代入函数中自变量的值，参数矩阵存储函数中的所有参数，第i列存储第i个假设函数的参数。二者相乘的结果得到的向量对应数据即为问题结果。题中得到的矩阵结果，第i列对应第i个假设函数的结果。

与矩阵向量乘法相似，这一公式常常用于编程语言中，它可以减少代码量，提高效率。

### 矩阵乘法特征

#### 交换律

不同于实数乘法和标量乘法，**矩阵乘法不服从交换律。**

![image-20220120113035528](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120113035528.png)

#### 结合律

与实数乘法和标量乘法相同，**矩阵乘法服从结合律。**

#### 单位矩阵

在线性代数中，也存在**与实数中的“1”地位相同的矩阵**，称作**单位矩阵**。

**单位矩阵一定是方阵。**

![image-20220120113230852](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120113230852.png)

对于任意矩阵A，将其和其对应的单位矩阵相乘，得到的结果都是原矩阵。

即与单位矩阵的相乘，服从交换律。

![image-20220120113401354](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120113401354.png)

### 逆和转置

#### 逆矩阵

线性代数中的逆矩阵，与**实数空间中的倒数运算**，地位相似。

和倒数一样，**不是所有的矩阵都有逆矩阵。**

不存在逆矩阵的矩阵称作“奇异矩阵”，可以把其理解为实数部分的“0”.

![image-20220120114429901](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120114429901.png)

![image-20220120113905630](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120113905630.png)

**存在逆矩阵的方阵和其逆矩阵相乘，结果为对应的单位矩阵。**

那么如何求解逆矩阵呢？利用上式可以手算，不过现在绝大部分编程软件都可以直接求解逆矩阵。

#### 转置：Transpose

行变列，列变行。

![image-20220120114704774](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220120114704774.png)

## 第四章：多变量/特征值的线性回归

### 多功能/多变量

#### 引入多变量

之前我们研究的假设方程只针对单一变量x：

![image-20220121110344862](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121110344862.png)

 现在我们加入多个变量/特征值，对其进行研究：

![image-20220121110709756](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121110709756.png)

上图含四个变量。
$$
n表示特征向量数目\\
x^{(i)}表示一个n维向量。\\
x_j^{(i)}表示第i个向量的第j个特征值的值。
$$

#### 多变量的假设方程

以往我们单变量的假设方程如下：

![image-20220121111643893](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121111643893.png)

结合线性特征和多维变量，易得多变量的假设方程如下：

![image-20220121111758079](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121111758079.png)

结合线性代数知识，我们可以将上式的表达方式进行简化：

![image-20220121111858620](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121111858620.png)

即可得多变量假设方程的通式：
$$
h_{\theta}(x)=\theta^Tx\\(x_0=1)
$$
其中θ和x分别为存储参数、变量值的（n+1）维向量。

### 多元梯度下降法

#### 多元代价方程

![image-20220121112405889](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121112405889.png)

上图中，我们将参数θ看作一个（n+1）维的向量，代价函数J看作向量θ的函数。

根据上图，将其代入梯度下降算法得：

![image-20220121112546673](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121112546673.png)

#### 单元梯度下降与多元梯度下降

左图为单元梯度下降，右图为多元梯度下降：

![image-20220121113227848](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121113227848.png)

其中最大的区别就是单元梯度下降使用x^i即可表示该变量的第i个特征值，而多元梯度下降则为x_j^i表示第j个变量的第i个特征值。

需要注意的是，**上图中左右两个算法是等效的、可相互替换的。可以替换的原因观察左右两边参数中的自变量的上下标即可发现。**

### 梯度下降：特征缩放（feature scaling）

#### 核心思想

**使特征值在一个特定、相似的范围内。**

#### 引例

依旧是房子出售的问题，我们引入两个变量：房子大小以及卧室数目，其范围如下图所示：

![image-20220121114211071](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121114211071.png)

如左图，由于范围过大，相比之下我们梯度下降每一次下降的距离过小，导致我们梯度下降算法效率低。

于是我们根据变量范围进行缩小（除去它们的最大值，让其范围在[0,1]之间），这时我们的梯度下降算法效率可以由很大的提升（见右图）。

#### 处理办法

##### 除以最大值

缩小变量范围是特征缩放常用方法之一，最好缩放所有变量大约在以下范围内：

![](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121115608929.png)

近似的范围也是可以的，但范围过小/过大都不可以：

![image-20220121115727735](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121115727735.png)

##### 均值归一化

用类似于标准差（减平均值除以方差）的形式代替原本的变量。

![image-20220121120948545](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121120948545.png)

实际上，我们取的**平均值只要大概估算**即可，也无需除以方差，**除以最大值与最小值的差值**即可。

![image-20220121121227756](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121121227756.png)

#### 作用

特征下降不必计算出很精确的值，其作用是令梯度下降算法运行的更快，收敛所需的迭代次数更少。

### 梯度下降：学习率

#### 本课目的

![image-20220121121810994](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121121810994.png)

#### 梯度下降正确工作的判断

##### 方法一：图像法

梯度下降的目的就是不断寻找更好的参数来寻找代价函数的最小值。

如下图，竖轴为代价函数的最小值，横轴为迭代次数。根据梯度下降目的可知，代价函数应该在每一次迭代后减小。得出的曲线一般如下图所示：

![image-20220121122737178](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121122737178.png)

以左侧第一个红点为例，其表示当迭代次数到达100次时，当前代价函数最小值为xxx，此时可确定一个参数。

**当逐渐平缓时（300-400），此时说明梯度下降已经寻找到了代价函数的全局最小值，梯度下降算法已经收敛。因为代价函数几乎不变，不再降低。** 梯度下降已执行完毕。

不同数据其梯度下降的迭代次数也不尽相同，可能30次、也可能是3000次，甚至30000次。

这种方法的好处还在于可以提前告知我们，梯度下降算法有没有正常工作：

![image-20220121123900675](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121123900675.png)

当图像像上图一样，代价函数随着迭代次数逐渐增加时，就可以说明梯度下降算法没有正常工作。这通常意味着我们应该使用较小的学习率α。因为学习率过大会造成下图的结果：

![image-20220121124102970](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121124102970.png)

跳过最小值，不断向上走。

还有一种情况，即代价函数先降低又增加、先降低又增加、循环往复，我们也应将学习率调小：

![image-20220121124254084](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121124254084.png)

![image-20220121124359010](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121124359010.png)

因为**对于足够小的学习率，每一次迭代代价函数都会减小。**如果没有减小，那么就是学习率不够小。但如果学习率太小，梯度下降算法就会很慢。

##### 方法二：自动收敛测试

该种方法是使用另一种算法告诉你梯度下降算法是否已经收敛。

![image-20220121123505609](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121123505609.png)

进行一次迭代，**如果代价函数的减小量过小（小于一个非常小的数），那么我们认为此时梯度下降算法已收敛。**

但通常选择一个合适的阈值（非常小的数）是很困难的，所以法一更常用。

#### 总结

- 学习率太小：梯度下降算法缓慢的收敛。
- 学习率太大：每一次迭代时，代价函数可能不会降低，导致梯度下降算法不会收敛。（也可能会缓慢收敛）
- 找到合适的学习率：可以以下图的方式尝试代入学习率（老师更建议每次乘3）

![image-20220121124803939](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121124803939.png)

### 特征和多项式回归

#### 特征

我们想出售一个房子，选取了两个特征（临街宽度和临街深度）来预测出售价格：

![image-20220121130029568](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121130029568.png)

但出售价格往往和房屋面积息息相关，所以如果我们把特征值换为面积：

![image-20220121130302771](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121130302771.png)

减少了特征值数量，还使其更贴近现实。

所以通过定义新的特征值来正确的选择特征值是很重要的。

#### 多项式回归

与选择特征值很贴切的一个概念：多项式回归。

比如说我们得到下图中的房屋模型：

![image-20220121130542212](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121130542212.png)

可能会有多个不同的模型进行拟合，比如直线模型可能不太拟合，二次模型可能会更贴合一些。

那么我们如何将模型和数据进行拟合呢？使用多项式回归的方法，我们可以对算法做一个简单的修改来实现它。

#### 总结

为了更贴近数据库，**选择合适的特征和假设函数（多元回归）**来确定更好的模型很必要。

![image-20220121130959832](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121130959832.png)

### 正规方程（normal equation）：区别于递归下降（迭代方法）的直接解法

正规方程与递归下降类似，其作用都是求解最好的参数让代价函数最小。

#### 引例

引入四个特征值和一个因变量，将自变量变为m*(n+1)的矩阵，因变量变为m维向量：

![image-20220121133030026](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121133030026.png)

最好参数即可求得：
$$
\theta =(X^TX)^{-1}X^Ty
$$

#### 递归下降与正规方程的区别

![image-20220121133931577](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121133931577.png)

与梯度下降相比，正规方程的优点：

- 不需要在操作之前，选择合适的学习率，以加快计算效率。
- 不需要迭代，画图来判断算法是否收敛，或者采取任何其他步骤。

![image-20220121134640619](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121134640619.png)

与梯度下降相比，正规方程的缺点：

- 当问题规模n太大（到达10^6次方）时，计算十分缓慢（因为计算所需的时间复杂度到达了`O(n^3)`）

#### 总结

只要**特征变量的数目不大**，正规方程是一个很好的计算参数θ的替代方法，但其在很多高级的学习算法中不能使用，所以梯度下降算法仍然是一个很好用的算法。

不过根据不同问题、特征变量数的不同，我们可以选择不同的方法，这两种方法都是值得学习的。

### 正规方程以及不可逆性

根据正规方程：
$$
\theta=(X^TX)^{-1}X^Ty
$$
但是，**有些矩阵是不可逆的**，在这种情况下，我们如何使用正规方程？

在Octave中，即使不可逆，使用伪逆运算函数`pinv()`，也可以得到θ：

![image-20220121135628378](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121135628378.png)

那么，脱离编译软件，我们下面从数学角度来解释一下如何解决不可逆问题。

首先，什么样的情况会导致矩阵不可逆？

1. 你学习的问题包含了多余的特征值。（如下图，第一个特征值是平方英尺，第二个特征值是平方米，这二者之间本身就存在关系：线性相关）                                                                        解决方法：减少相关的特征值。
2. 你在使用的学习算法引入了太多特征值。（如下图：m（样本数量）小于等于n（特征数量），样本数过小，导致无法解决你想要的问题规模）                                                                     解决方法：删除一些特征值/使用正规化（后续课程会讲解）。

![image-20220121135800706](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220121135800706.png)

## 第五章：Octave编程语言

### 基本操作

#### 四则运算|与或非

![image-20220122104609237](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122104609237.png)

特殊：octave中的不等于：`~=`

#### 输出

![image-20220122104727037](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122104727037.png)

特殊：一般输入赋值语句/变量，会输出该语句/变量的值。

例如：

```octave
输入：a = 3
输出：a = 3
输入：a
输出：a = 3
```

但在赋值语句后加 `;` 则不会输出。

例如：

```octave
输入：a = 3;
继续输入
```

![image-20220122105211867](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122105211867.png)

以上为C语言的旧语法，在octave中也可以进行输出。

#### 矩阵和向量

![image-20220122105350187](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122105350187.png)

输入语句中的`;`表示换入到下一行。

![image-20220122105530820](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122105530820.png)

以下的输入语句很有用：

```octave
v = 1:0.1:2
```

这一语句的含义是，从1出发，0.1为增量，输出增加到2的所有值。v将成为一个行向量。

结果如下所示：

![image-20220122105758465](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122105758465.png)

如果输入以下的语句：

```octave
v = 1:6
```

这一语句的含义是，从1出发，以1为增量，输出增加到6的所有值。v将成为一个行向量。

结果如下所示：

![image-20220122105939901](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122105939901.png)

生成矩阵还有其他方法：

```octave
ones(2,3)  %生成一个元素全部为1的2×3维的矩阵
```

结果如下：

![image-20220122110133870](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122110133870.png)

还可以进行乘积：

```octave
c = 2*ones(2,3)    %生成一个元素全部为2的2×3维矩阵
```

结果如下：

![image-20220122110243348](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122110243348.png)

以下代码也同理：

```octave
w = zeros(1,3)    %生成一个元素全部为0的1×3维矩阵
z = rand(1,3)     %生成一个元素全部为随机值的1×3的矩阵，且其元素范围皆在（0,1）
```

结果如下：

![image-20220122113743645](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122113743645.png)

服从高斯分布（正态分布）的随机数：

```octave
w = randn(1,3)
```

结果如下：

![image-20220122123851830](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122123851830.png)

它们的标准差/方差为1.

更复杂的随机函数使用：

```octave
w = -6 + sqrt(10)*(randn(1,10000))   %生成10000个数据，其服从正态分布，均值为-6，方差为10
hist(w)                   %将w绘制为直方图
hist(w,50)                %把直方图分为50条
```

结果1：

![image-20220122124236691](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122124236691.png)

结果2：

![image-20220122124340379](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122124340379.png)

生成单位矩阵的指令：

```octave
eye(4)      %生成一个4*4维的单位矩阵
```

结果如下：

![image-20220122124546290](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122124546290.png)

最后介绍帮助文档`help`：

```octave
help rand           %显示rand的帮助文档
help eye            %显示eye的帮助文档
help help           %显示help的帮助文档
```

#### 退出octave

```octave
quit   %输入后键入回车，即可退出octave（exit也可以）
```



### 移动数据

移动数据：把机器学习中的数据加载到octave中。

#### 矩阵相关操作

##### size函数/length函数：

```octave
A =[1 2;3 4;5 6]     %输入矩阵
size(A)              %计算矩阵的尺寸，返回一个1*2的矩阵
sz=size(A)
```

size结果如下：

![image-20220122130311553](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122130311553.png)

![image-20220122130354604](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122130354604.png)

还可以在size函数中加入更多参数：

```octave
size(A,1)       %计算矩阵A第一维度的大小，返回一个数
size(A,2)       %计算矩阵A第二维度的大小，返回一个数
```

结果如下：

![image-20220122130627430](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122130627430.png)

同样的，有相似作用的length函数：

```octave
v = [1 2 3 4]
length(v)           %返回较大维数
length(A)           %返回较大维数
```

结果如下：

![image-20220122131432569](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122131432569.png)

#### 载入文件

##### 更改路径

显示当前路径：

```octave
pwd
```

![image-20220122140116457](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122140116457.png)

在当前界面下，更改路径：

```octave
cd '路径名'
```

![image-20220122140231850](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122140231850.png)

显示桌面路径：

```octave
ls
```

![image-20220122140329478](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122140329478.png)

##### 载入文件

载入已有文件：

```octave
load 文件名
load('文件名的字符串')
```

![image-20220122140514813](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122140514813.png)

显示octave中所有变量：

```octave
who     %octave内存中当前存储的所有变量
featuresX  %显示featuresX文件中的数据
size(featuresX)   %显示featuresX数据的矩阵大小
```

![image-20220122140644019](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122140644019.png)![image-20220122140833904](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122140833904.png)

显示变量更详细的信息：

```octave
whos
```

![image-20220122141005929](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122141005929.png)

删除变量：

```octave
clear 变量名
clear featuresX %删除featuresX这一变量，显示变量时featuresX将不存在
clear    %删除内存中所有变量
```

存储数据：

```octave
v = priceY(1:10)   %将向量Y的前十个元素全部赋给v
save hello.mat v   %使变量v保存为一个名为hello.mat的二进制文件
save hello.mat v -ascii  %存储成文本文档/ascii编码的文档
```

#### 操作数据

##### 索引

```octave
A = [1 2;3 4;5 6]
A(3,2)    %索引到A的第三行第二列的元素
A(3,:)    %索引A的第三行的所有元素
A(:,2)    %索引A的第二列的所有元素
A([1,3],:)    %索引A的第一行和第三行的所有元素
```

![image-20220122141743989](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122141743989.png)

![image-20220122141833674](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122141833674.png)

![image-20220122141914422](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122141914422.png)

![image-20220122142055265](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122142055265.png)

##### 赋值

```octave
A(:,2)= [10;11;12]  %用右侧向量赋值到矩阵A的第二列
```

![image-20220122142240186](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122142240186.png)

```octave
原矩阵 = [原矩阵,列向量]   %给原矩阵附加新的一列
A = [A,[100;101;102]]     %给矩阵A附加新的一列
```

![image-20220122142527463](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122142527463.png)

##### 整理为列向量

```octave
A(:)   %将矩阵A中的所有元素放到一个新的列向量中
```

![image-20220122142654875](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122142654875.png)

##### 合并矩阵

```octave
A=[1 2;3 4;5 6]
B=[11 12;13 14;15 16]
C=[A B]   %合并矩阵A（在左边）和矩阵B（在右边）赋给矩阵C
C=[A;B]   %合并矩阵A（在上边）给矩阵B（在下边）赋给矩阵C
```

![image-20220122142957995](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122142957995.png)

![image-20220122143210692](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122143210692.png)

### 计算数据

#### 基本运算

```octave
A*B   %矩阵乘法
A.*B  %将矩阵对应元素进行相乘     .表示矩阵元素之间的运算
A.^2  %将矩阵A中的元素进行平方计算
1./A  %求取矩阵A中所有元素的倒数
log(A)%对矩阵A中所有元素取对数
exp(A)%以矩阵A中所有元素为指数的幂运算
abs(A)%对矩阵A中所有元素取绝对值
-A    %对矩阵A中所有元素取相反数
A + 1 %对矩阵A中所有元素都+1
A'    %求得A的转置
```

![image-20220122143517375](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122143517375.png)

![image-20220122143656795](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122143656795.png)

```octave
max(A)  %返回矩阵A中每一列的最大元素
max(a)  %返回向量a中的最大元素
```

![image-20220122144509263](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122144509263.png) 	

```octave
a < 3  %将a中所有元素与3比较，返回比较后的真假值（0/1）
```

![image-20220122144636295](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122144636295.png)	

```octave
find(a<3)  %返回a中所有小于3的元素的索引
```

![image-20220122144736556](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122144736556.png)

```octave
A = magic(3) %生成一个三维幻方矩阵（每行每列每个对角线上的元素和都相等）(基本不会用到)
[r,c] = find(A>=7) %返回矩阵A中大于等于7的元素的行列值
```

![image-20220122144910615](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122144910615.png)

![image-20220122145058730](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122145058730.png)

```octave
sum(a)     %向量a中所有元素求和（也可以求解矩阵）
sum(A,1)   %矩阵A每一列的总和
prod(a)	   %向量a中所有元素求乘积
floor(a)   %向量a中所有元素向下取整
ceil(a)    %向量a中所有元素向上取整
```

```octave
max(A,[],1) %求取矩阵A中每一列的最大值
max(A,[],2) %求取矩阵A中每一行的最大值
max(max(A)) %求矩阵A中的最大值（max默认求解每一列的最大值）
```

![image-20220122145454426](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122145454426.png)

![image-20220122145536231](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122145536231.png)

```octave
pinv(A)   %对矩阵A进行求逆运算
```

### 数据绘制

```octave
plot(x,y);   %根据变量x,y绘制一个图，以x为横轴，y为纵轴
```

![image-20220122150314153](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122150314153.png)

![image-20220122150327801](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122150327801.png)

继续使用plot函数的话，会消除之前的图像，重新绘制一个新的图像。

```octave
hold on;   %让octave在旧的图像基础上绘制新的图像
plot(x1,y1,'r');  %在原有图像基础上以红色绘制新图像
```

![image-20220122150611226](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122150611226.png)

```octave
xlabel('time')   %给横轴赋予含义time
ylabel('value')  %给纵轴赋予含义value
legend('sin','cos')  %赋予图例
title('my plot')   %赋予标题
print -dpng 'myPlot.png'   %保存图像为png形式的文件
close   %关闭图像
figure(1);plot(x,y);   %标识图像名
subplot(1,2,1)   %给图像划分1×2的小格子，生成一个图
plot(x,y)   %将图像加入到划分的格子中
axis([0.5 1 -1 1])  %改变图像中轴的刻度，可以改变x轴y轴刻度的划分
```

![image-20220122150850008](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122150850008.png)

subplot

![image-20220122151418222](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122151418222.png)

![image-20220122151401229](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122151401229.png)

axis

![image-20220122151607907](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122151607907.png)

```octave
clf;  %清除图像
imagesc(A)   %可视化矩阵A，不同颜色对应不同的矩阵元素值
colorbar    %显示色条和刻度，用于表示矩阵A中生成的图像颜色代表的数值
imagesc(A),colorbar %,可以连接多个命令语句，让其同时执行
```

imagesc

![image-20220122151741136](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122151741136.png)

### 控制语句：for,while,if语句

#### for

![image-20220122152155614](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122152155614.png)

其中i表示从1到10进行遍历，注意要加入**end**。

![image-20220122152318493](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122152318493.png)

#### while

![image-20220122161925578](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122161925578.png)

循环中`break`和`continue`的用法和其他编程语言一致。

![image-20220122162030623](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122162030623.png)

需要注意的是，上式中的第一个`end`用于终止`if`语句；第二个`end`用于终止`while`语句。

#### if

![image-20220122162212750](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122162212750.png)

整个`if`语句后需要加入`end`来终止。

#### 调用函数

首先，在调用函数之前，我们需要定义一个函数：

![image-20220122162622282](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122162622282.png)

将其存储到文件之中。

回到octave中，如果直接进行调用会出现错误提示，因为octave不知道在哪里对这个函数中进行调用。

![image-20220122162750089](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122162750089.png)

所以我们需要先使用pwd，更改octave路径至该函数的存储位置，让其可以寻找到该文件。

![image-20220122162837133](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122162837133.png)

当然，还有更高级的方法，提前加入你常存储函数的路径到octave的搜索路径中，这样在octave直接搜索就可以查询到。

![image-20220122163511145](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122163511145.png)

与其他编译语言不同的是，octave支持你定义一个返回多个值的函数：

![image-20220122163640583](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122163640583.png)

![image-20220122163702637](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122163702637.png)

而这一步我们在机器学习中也可以用到，比如定义一个代价函数：

![image-20220122163857030](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122163857030.png)

![image-20220122164040459](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122164040459.png)

### 矢量

许多编程语言（包括octave）都支持线性代数库，而这些线性代数知识，在参数量很大时，相比于利用代数循环，更有助于我们快速解决问题。所以能够向量化问题，是我们必备的基础素质之一。

如下图，就是利用线代知识，将一些参数进行向量化，解决问题：

![image-20220122165449366](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220122165449366.png)

## 第六章：logistic regression

### 分类

#### 定义

何为分类？

![image-20220123090938777](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123090938777.png)

像上图中，答案是离散的，即为可分类的。例如邮件中有广告邮件/无广告邮件；欺诈交易/非欺诈交易；恶性/良性肿瘤……等等类似这种的问题，在这些问题中**答案y只有两种取值：0或1.（二分类）**

其中0表示负类，1表示正类。它们包含了问题的具体答案：例如0表示良性肿瘤，1表示恶性肿瘤。其表示的含义可以任意互换，不过我们经常把**0意为没有xxx，而1意为有xxx。**

进一步拓展，还有多分类的形式，y的取值范围将会更多：
$$
y∈\{0,1,2,……,n\}
$$

#### 解决问题

##### 在线性回归中使用分类

![image-20220123092606103](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123092606103.png)

在只考虑前八个数据元素时，我们画出其假设函数的图像（粉线），观察图像设置分类阈值：0.5。

此时大于等于阈值的都为恶性肿瘤，小于阈值的都为良性肿瘤。这时分类能贴合现实数据。

加入第九个数据元素后，由于变量的增加，假设函数也随之改变，显然此时原阈值的分类已不符合现实数据。

所以在解决问题中，**不建议在线性回归中使用分类。**

##### 在分类中使用线性回归

![image-20220123092149781](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123092149781.png)

但是有一个关键问题：假设函数的值可能大于1或小于0，这种情况就很难将其分类。

为了解决该问题，我们引入`logistic regression`算法，其属于分类，它可以将假设函数值限制在[0,1]中。

![image-20220123092339493](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123092339493.png)

### 假设陈述：在logistic regression中假设函数的表示方法

sigmoid/logistic regression函数：
$$
g(z)=\frac{1}{1+e^{-z}}
$$
假设函数：
$$
h_\theta(x)=\theta^Tx
$$
将假设函数代入到logistic函数中：
$$
g(h_\theta(x))=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
$$
绘制图像：

![image-20220123093931870](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123093931870.png)

当z趋近于负无穷时，g(z)趋近于0；z趋近于正无穷时，g(z)趋近于1.

所以可以保证假设函数在[0,1]的范围内。

假设函数输出的解释：

![image-20220123100444354](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123100444354.png)

### 决策边界：decision boundary

在logistic regression中，何时y=1/y=0呢？对此我们有概率的设定值。

![image-20220123100838761](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123100838761.png)

当假设函数大于等于0.5（即发生概率）时，我们认为y=1（因为更可能发生），反之我们认为y=0.

那么如何判断其大于等于0.5/小于0.5呢？

观察下个图象：

![image-20220123101131232](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123101131232.png)

不难发现当g(z)>=0.5时，z>=0，此时y=1；反之g(z)<0.5，z<0，此时y=0.

这里的z等于θ^Tx.

**总结：当θ^Tx大于等于0时，我们可以预测y=1；反之预测y=0.**

#### 例题

![image-20220123101643652](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123101643652.png)

根据上式中参数，想要预测y=1，即令-3+x1+x2>=0。解得x1+x2>=3，在图像表示为一条直线。

绘制加入图中，可以发现，这条线将y=1和y=0的两种情况分隔开。**这条线即为决策边界，在这条线上假设函数恒等于0.5**

以上为线性决策边界，当然假设函数问题维度提高时也存在非线性决策边界：

![image-20220123102738309](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123102738309.png)

**最后，决策边界不是数据集的属性，而是假设本身及其参数的属性。所以一旦确定了参数值，决策边界也随之被确定。**

### 代价函数

![image-20220123103143099](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123103143099.png)

问题：对于给定的数据集，如何选择/拟合参数Θ？

答：使用代价函数。

在我们前期学习的是线性代价函数，但由于我们的假设函数不再服从线性规律，它更复杂，所以直接套用线性代价函数会导致如左下结果：

![image-20220123104110344](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123104110344.png)

其是非凸函数，这时我们再使用梯度下降，有可能无法找到全局最小值。

所以我们的目的是找到一个更好的代价函数，让其成为右侧的凸函数。

于是，我们对logistic regression设置新的代价函数：

![image-20220123103923606](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123103923606.png)

当y=1时，如果我们预测假设函数也等于1，那么代价为0，表明预测正确；但若我们预测假设函数等于0，代价将无穷大，此时我们会惩罚机器学习算法。

![image-20220123104304277](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123104304277.png)

y=0时同理。

### 简化代价函数和梯度下降

上节中我们得到logistic regression的代价函数分为了两部分，为了使其更紧凑，我们采用其通式：

![image-20220123105003786](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123105003786.png)
$$
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
$$
将该式代入到函数J中：

![image-20220123105219655](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123105219655.png)

接下来我们要做的是，最小化代价函数来得到一个最佳参数θ，然后输入样本来进行预测：

 ![image-20220123105327857](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123105327857.png)

那么，我们接下来的问题是如何最小化代价函数？

执行梯度下降算法：

![image-20220123105651535](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123105651535.png)

表面上似乎对logistic regression和对线性回归执行梯度下降算法一致，但事实上，二者的假设函数已然改变：

![image-20220123105818813](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123105818813.png)

### 高级优化

以下将介绍一些相比梯度下降算法更能高效解决logistic regression问题的算法、以及其更适合用于大型的机器学习。

除了梯度下降算法可以求解`minJ(θ)`，还有以下三种算法，它们优于梯度下降算法，但更复杂。无需理解原理，会使用即可。

![image-20220123125925506](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220123125925506.png)

#### 高级优化算法的基本使用

![image-20220124094109499](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124094109499.png)

一式中的`jVal`指代价函数，`gradient`指一个n*1维的向量（n为参数数量）。在下式中可看出，其与参数相对应，其式为对应参数的偏导。

![image-20220124094406499](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124094406499.png)

执行以上操作后，我们就可以开始调用高级函数`fminunc`（无约束最小化函数）了。调用方法如下：

![image-20220124094653511](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124094653511.png)



首先要设置几个`options`（变量，可以存储任意你想要的options的数据结构）

`GrabObj`和`on`意味着要将梯度设置打开，`MaxIter`和`100`为设置迭代最大值，比方说100.

`InitialTheta`给θ一个猜测初始值，它在这个问题中为2*1维的矩阵。

最后一句语句调用`fminunc`.`@`意为指向代价函数指针。接下来执行这些代码，程序就会自动我们使用最合适的高级优化方法。

在octave中的实现：

![image-20220124095609397](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124095609397.png)

得到最优参数θ1=5,θ2=5.

`functionVal`意为代价函数值，这里是10^-30次，近似等于0.

`exitFlag`为1，代表已收敛。

需要注意的是，在使用高级算法过程中，`initialTheta`的维数必须在二维及以上，如果输入的参数仅仅为一个实数（一维），那么这一算法将无法解决问题。

#### 小结

![image-20220124100358258](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124100358258.png)

### 多元分类：一对多

#### 多元分类

![image-20220124100552157](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124100552157.png)

以上，y具有多个离散值对应，这样的分类问题称为多分类。

![image-20220124100719827](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124100719827.png)

#### 一对多

解决一对多的问题，方法是将其多次分为二分类。一次只考虑两个变量。

![image-20220124101009189](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124101009189.png)

如上图，第一次分析只考虑一类数据（y=1）和非一类数据（y=0）；第二次分析只考虑二类数据（y=0）和非二类数据（y=1）……以此类推。

然后逐一分析其y=1的概率，最后进行结合。

#### 小结

![image-20220124101451669](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124101451669.png)

让假设函数h(i)对应分类问题中y=i时的概率，在输入新的数据时，来进行预测，选择出可能性最大的分类。

## 第七章：正则化（regularization）

### 过拟合问题

减少过拟合问题有助于优化我们的算法，利于解决问题。那么什么是过拟合问题？

#### 引例

![image-20220124102440882](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124102440882.png)

图一直接使用线性函数反映数据，但从图中可以看出其**几乎不拟合实际数据，有很大偏差，这种情况我们称为“欠拟合”。**其产生原因是，几乎不考虑实际数据的变化趋势，一味盲目使用线性函数。

图二使用二次函数反映数据，与实际数据拟合很好。处于“欠拟合”和“过度拟合”之间，这种情况我们认为刚刚好，很合理。

图三使用四次函数反映数据，虽然经过了所有数据，但不断地上升下降，我们不认为这是个好的拟合模型，这种情况我们称为“过度拟合”或称为“具有高方差”。其产生原因是，**如果我们拟合一个高阶多项式，那么这个多项式可以贴合所有数据，可能导致太过庞大、变量太多的问题，导致我们没有一个足够数据来约束它来获得一个好的假设函数。这就是过度拟合。**

![image-20220124111727335](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124111727335.png)

`generalize`是指一个假设函数应用到新样本的能力，如果过度拟合，该能力会很差。

在多变量问题中也同理：

![image-20220124111940917](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124111940917.png)

#### 解决办法

当我们设置的特征量过多而实际数据过少时，容易出现过拟合问题。

所以解决办法就是

- 减少特征量（人为选择哪些变量保留/机器选择算法）：缺点是一旦你舍弃了特征量，其对应信息也将舍弃。
- 正则化（regularization）：保留所有特征值，减小参数来避免过拟合。

### 代价函数

#### 引例

下图中左侧为合适的模型，右侧为过拟合。那么如何将过拟合模型转变的更加合适呢？

![image-20220124113121257](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124113121257.png)

在代价函数中加入“惩罚”（如图中式子），要想减小代价函数，需要让θ3,θ4非常小，近似等于0.

此时，该假设函数近似等于二次函数，更加合适。

![image-20220124114029957](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124114029957.png)

#### 正则化

从引例当中可以看出正则化的思想：

![image-20220124114116785](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124114116785.png)

使参数尽可能小，有助于简化假设函数，避免过拟合。但在诸多参数中，我们往往不知道缩小哪个参数能使模型更合适，所以我们缩小所有参数。

那么在代价函数中如何操作呢？

![image-20220124114216443](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124114216443.png)

在原代价函数中加入一项：
$$
\lambda\sum_{i=1}^n\theta_j^2
$$
这一项的作用是缩小每一个参数。（约定俗成从θ1开始缩小，不缩小θ0。当然缩小θ0也没什么影响）其中的**λ**被称为**正则化参数**，其作用是**控制两项之间的平衡关系，从而避免出现过拟合的情况。**

第一项：
$$
\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})^2)
$$
作用：尽可能贴合实际数据。

第二项：
$$
\sum_{i=1}^n\theta_j^2
$$
作用：保持参数尽可能小。



但如果**正则化参数λ被设置的过大，对于所有参数惩罚过强，导致所有参数都近似等于0。此时假设函数等于θ0，不能很好的贴合现实数据。**

![image-20220124115344634](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124115344634.png)

所以正则化选取一个合适的正则化参数是很重要的。

### 线性回归的正则化

#### 正则化的梯度下降

与普通线性问题的梯度下降类似，正则化的梯度下降没有太大变化：

![image-20220124115938829](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124115938829.png)

由于正则化中我们不减小θ0，所以θ0的梯度下降与线性问题中一致。

其他减小的所有参数，比线性问题时多增加了一项偏导数。

稍微整理后：

![image-20220124120233162](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124120233162.png)

其中
$$
1-\alpha\frac{\lambda}{m}
$$
恒小于1.

所以正则化梯度下降每一次迭代时，就是将所有参数缩小一点（乘以一个略小于1的数），然后执行和线性问题相同的更新操作。

#### 正则化的正规方程

线性问题的正规方程：
$$
\theta=(X^TX)^{-1}X^Ty
$$
正则化的正规方程：
$$
\theta=(X^TX+\lambda A)^{-1}X^Ty
$$
其中矩阵A为(n+1)维方阵，等于：

![image-20220124120825998](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124120825998.png)

那么如果正则化的正规方程中的矩阵不可逆，我们应该如何处理？

答案是，只要正则化参数大于0，那么其就不可能不可逆。

![image-20220124121219998](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124121219998.png)

### logistic regression的正则化

一般logistic regression的代价函数：

![image-20220124133134710](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124133134710.png)

logistic regression正则化的代价函数：

![image-20220124133213662](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124133213662.png)

梯度下降：

![image-20220124133316355](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124133316355.png)

高级优化：

![image-20220124133726442](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220124133726442.png)

## 第八章：神经网络——表示（Neural Network: representation）

### 非线性假设

计算机对于图像的处理与我们人类视觉呈现大不相同。如下图，我们放大汽车车把手的部分的得以了解，这是汽车的车把手，但对于计算机来说，它看到的是一个数据矩阵。所以需要我们人类提前告知，这一矩阵代表车把手。

![image-20220125100609595](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125100609595.png)

机器判断图片内容是否是汽车时，需要多个变量，即转化为非线性问题，有时问题的时间复杂度高达二维、三维甚至更多，而按照之前的方法进行非线性假设，当问题的规模过大时，其解决速度非常慢。

![image-20220125101143595](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125101143595.png)

所以，神经网络的作用就是解决复杂的非线性假设。

### 神经元与大脑

要知道，一开始建立人工智能的目的，就是希望用机器来建设一个大脑，像人一样的思考。

神经网络起源：

![image-20220125101609103](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125101609103.png)

人类大脑的学习机制十分神奇。如果将听觉神经斩断，将视觉神经连接于斩断的听觉神经位置，那么你的耳朵就学会了“看”。

### 模型展示Ⅰ

人工智能中的神经网络与人脑类似，以下为处理logistic regression问题的模型：

![image-20220125113401512](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125113401512.png)

输入信息，经过“大脑”进行整合，输出假设函数结果。以上为单神经元工作机制。

引入多个神经元就构成了神经网络：

![image-20220125114053376](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125114053376.png)

![image-20220125114709695](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125114709695.png)

### 模型展示Ⅱ

计算假设函数步骤：

![image-20220125115915027](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125115915027.png)

本质：logistic regression

![image-20220125120416932](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125120416932.png)

![image-20220125120433336](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125120433336.png)

### 例子与直觉理解Ⅰ

这一节中，我们来认识一下神经元是如何进行逻辑运算的。

#### 引例1：与运算

研究两个变量，函数y：这两个变量的与运算。设置如下参数，绘制假设函数图像。

将变量所有真值代入，得到如下真值表：

![image-20220125121601078](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125121601078.png)

发现结果和与运算类似，我们可以认为假设函数就是进行与运算的函数。

#### 引例2：或运算

与上一例子相似的：神经元也可以进行或运算：

![image-20220125123525001](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125123525001.png)

根据上面两个例子，可以看出，单个神经元可以进行逻辑运算（与运算/或运算等）。

### 例子与直觉理解Ⅱ

这一节中，我们来认识一下，神经元是如何处理更复杂的非线性假设模型的。

我们想求解两个变量同或的真值表。

神经网络处理这个问题的步骤如下：

![image-20220125124507735](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125124507735.png)



- 将异或拆分为3个基础的小问题。
- 简单的问题由两个神经元结合解决后，送至下一神经元。
- 神经元接收问题，最后输出运算结果给假设函数。

这就是神经网络可以解决复杂的非线性假设模型的原因。**将十分困难复杂的问题拆分成一个个简单的子问题，每个神经元负责其中一个，所有神经元结合起来形成神经网络就能解决复杂的问题。**

![image-20220125125037508](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125125037508.png)

最后我们来介绍神经网络解决复杂问题的一个实例：手写数字识别。

![image-20220125125325688](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125125325688.png)

其中左侧图片即为神经网络分析的内容，第一个隐藏层（指神经元的分析过程，在使用过程中不会显示出输出的一部分称作隐藏层）分析出复杂的特征；第二个隐藏层分析出更复杂的特征；第二个隐藏层分析出更复杂的特征……以此类推，直至最后分析出我们想要的结果（假设函数），判断数字。

### 多元分类

要在神经网络中实现多元分类，采用的方法本质上是对一对法的拓展。

#### 例子　　　　　　　　　　　　　　　

识别下列图片，它们可能是：行人、汽车、摩托车、卡车。

这就反映了一对多的问题：传入一个照片，其有多种可能的结果。

神经网络的处理方法就是设置多个作为逻辑回归分类器的神经元，一个判断是否是行人？一个判断是否是汽车？一个判断是否是摩托车？一个判断是否是卡车？

![image-20220125130010586](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220125130010586.png)

具体的判断方法就是看经过神经元分析出的结果（假设函数）：其结果为矩阵形式，如上图结果所示。

## 第九章：神经网络——学习

### 代价函数

神经网络分类：左侧为简单的一对一问题，右侧为一对多。

![image-20220126102623046](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126102623046.png)

代价函数：二者的区别就是，神经网络将问题拓展到了K阶（K维向量），i表示输出该向量中的第i个元素。

![image-20220126102742467](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126102742467.png)

### 反向传播算法

在上一节中我们介绍了神经网络的代价函数，接下来我们为了求解最好的参数，需要通过梯度下降或高级优化算法来求代价函数的最小值。使用这些算法求解最小值之前需要求出两个值：

- 代价函数
- 代价函数对参数的偏导

第一步由通式即可求解，如何求解第二步则是我们这一节的内容。

![image-20220126103454826](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126103454826.png)

接下来，给一个一维问题（实数）的例子，神经网络处理过程如下：

![image-20220126103903330](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126103903330.png)

#### 算法内容

反向传播方法的内容如下：

![image-20220126104300845](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126104300845.png)

引入一个变量：
$$
\delta_j^{(l)}
$$
它表示第l层第j个变量的误差（假设的输出值减去实际的输出值）。对于除输入层（第一层），其他所有层都存在误差。误差求法如上图。

反向传播算法核心就是**从输出层开始计算误差，然后我们回到上一层计算，再回到更上一层计算……以此类推。它的含义就是从输出层开始反向传播。**

最后经过反向传播算法，我们很容易能得到偏导数：

![image-20220126104759685](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126104759685.png)

#### 具体实现

![image-20220126105235387](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126105235387.png)

其中红色字为左式的向量化公式。

在执行以上的所有算法后可以得到：

![image-20220126105320877](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126105320877.png)

接下来我们就可以使用梯度下降或其他高级优化算法来计算最小的代价函数。

### 理解反向传播

#### 前向传播

下图表示神经网络前向传播（从左到右）工作的流程：

![image-20220126112605771](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126112605771.png)

可以看出前向传播中从输入端开始由上一级计算下一级。

反向传播与其非常类似，只不过传播方向发生改变（从右到左）.

为了更好地理解反向传播，我们先引入代价函数（单个变量）：

​	![image-20220126112842209](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126112842209.png)

其中cost(i)可近似看成方差，它代表了神经网络预测数据的准确度。

以下为反向传播的流程：

- 计算出输出端的误差δ4
- 结合下述公式求解误差δi

$$
\delta^{(i)}=\sum(权值\theta*\delta^{i+1})
$$

![image-20220126113548407](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126113548407.png)

所以反向传播是从输出端开始由下一级计算上一级。

### 展开参数：由矩阵展开为向量

在高级优化算法中，其参数要求是向量，但在神经网络中的参数以矩阵方式存储（便于前向/反向传播）。所以我们需要掌握展开参数的方法。

#### 引例

![image-20220126115025946](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126115025946.png)

`thetaVec`和`DVec`就是展开的向量，`Theta1` `Theta2` `Theta3`是10*11维的矩阵。

注意：`Theta1(:)`就是将矩阵`Theta1`化为一个列向量。

![image-20220126115707747](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126115707747.png)

### 梯度检测：检测前向/反向传播

神经网络进行前向/反向传播时，其很容易产生bug，导致虽然每次梯度下降时代价函数都在减小，但是最终值会比无bug时大很多。为了消除这类bug，我们有必要进行梯度检测。

#### 具体实现

第一步：求解导数近似值（几何含义）

当问题为一维，参数为实数时：

![image-20220126120556908](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126120556908.png)

当问题为多维，参数为向量参数时：

![image-20220126120753831](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126120753831.png)

代码实现：

![image-20220126120900259](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126120900259.png)

第二步：将反向传播求解出的偏导值和导数近似值相比，如果这二者相等或十分近似，那么说明反向传播的实现是正确的。

![image-20220126121035074](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126121035074.png)

#### 小结

![image-20220126121407793](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126121407793.png)

需要注意的是，**在进行多次梯度下降之前一定要关闭梯度检测，因为梯度检测的执行速度很慢，极其影响效率。**

### 随机初始化：对参数的初始化

![image-20220126121738769](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126121738769.png)

对于所有优化算法（高级优化、梯度下降）我们都需要对参数进行初始化，一般默许初始化都设为0，但在训练网络中，这种设置起不到任何作用。

以神经网络训练为例：

![image-20220126122009996](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126122009996.png)

因为参数都为0，所以前向/反向得到的活化和误差都相等（都在计算同一特征，最后的输出也只能得到一个特征，高度冗余），导致最终的偏导数结果也相等，即最终得到的参数结果相等。

![image-20220126122329552](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126122329552.png)

我们使用随机初始化来解决这个问题：

初始化每个参数为范围一定的某个随机数。实际做法如下：

![image-20220126122559541](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126122559541.png)

所以为了有效训练，我们需要**通过随机初始化将参数初始化至非常接近0，然后进行反向传播、梯度检验，最后执行梯度下降或者其他高级优化来最小化代价函数。**

### 组合

在这一部分，我们将前面学习的几部分整合起来，系统性规范性的总结神经网络的操作。

#### 架构

首先，对于神经网络，我们要先进行架构（选择一个神经网络结构），一般我们选择的结构有以下三种：

![image-20220126123140296](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126123140296.png)

关于神经网络结构涉及到以下的概念：

![image-20220126123440651](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126123440651.png)

输入量（特征量个数）、输出量（问题规模）、隐藏层的数目（单层或多层，一般初始设为单层，每一层的隐藏单元数相同。通常情况下，隐藏单元数越多越好，但计算量会被比较大）。

关于输出量需要注意的是，当你的问题规模已经为多维时，结果我们不再用集合表示，而是用向量：

![image-20220126123655151](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126123655151.png)

例如你的问题规模为10，获得的答案为5，不再写y=5，而是用十维向量其余元素为0第五个元素为1表示。

#### 训练神经网络步骤

![image-20220126124512607](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126124512607.png)

![image-20220126124629260](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126124629260.png)

1. 随机初始化权重（参数）。
2. 使用前向传播求解结果y。
3. 计算代价函数。
4. 使用反向传播求解误差、偏导数。
5. 使用梯度检测检测反向传播是否含有bug，确定无bug后关闭梯度检测。
6. 结合反向传播求出的偏导数进行梯度下降/高级优化算法去最小化代价函数。

需要注意的是，梯度下降/高级优化算法最后找出的代价函数最小值，只是局部函数最小值，可能不是全局函数最小值。但这没什么影响，最终找出来的代价函数性能也很好。

梯度下降/高级优化算法工作原理：

![image-20220126125503108](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220126125503108.png)

## 第十章：机器学习的使用建议

### 决定下一步做什么

修改学习算法，使预测值更加贴近实际情况有以下几种方法：

![image-20220127095930706](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127095930706.png)

但其中很多方式是无效费时的，接下来，我们介绍一个简单有效的方法：

![image-20220127100254777](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127100254777.png)

这一方法是比较耗时的，但它把时间用在了刀刃上，提前判断你的算法好坏、是否贴合，以免你在错误的道路上耗时太久。

### 评估假设

过拟合问题：

![image-20220127100919741](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127100919741.png)

如何解决？

随机打乱所有数据，取出前百分之七十作为训练集，后百分之三十作为测试集。

![image-20220127100944878](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127100944878.png)

- 线性回归问题

1. 从训练集中获得最优参数
2. 将最优参数代入到训练集的假设函数中，求解误差。

![image-20220127101132005](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127101132005.png)

- 分类问题

![image-20220127101431402](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127101431402.png)

### 模型选择和训练、验证、测试集

上一节中我们将所有数据分为两类：训练集和测试集，通过对其代价函数与假设的求解，我们能求出其很贴合我们训练集和测试集的数据。但需要注意的是，**根据已有数据求解、预测值十分贴合已有数据的假设函数，不代表能很好的预测新数据。**所以我们引入验证集，目的是为了更好的预测新数据。

随机打乱所有数据，通常，**训练、验证（cv）、测试集的数据占比分别为60％、20％、20%。**

![image-20220127103054402](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127103054402.png)

训练、验证、测试误差：

![image-20220127103353391](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127103353391.png)

所以**我们要使用验证集来选取模型（假设函数规模），而不是测试集。**

具体操作方法如下：

先利用一到十维的假设函数模型，求解最小化其代价函数，得到对应的最佳参数。代入到验证集的代价函数中，一一比较，选出最小的代价函数、其对应的假设函数即为我们可以选取的最优的模型。

![image-20220127103758270](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127103758270.png)

### 诊断偏差和方差

偏差过大，会导致欠拟合问题；方差过大，会导致过拟合问题。

下图表示了过拟合和欠拟合的不同情况：

![image-20220127105515833](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127105515833.png)

对于训练集来说，d越大越贴近训练集数据；对于验证集来说，d过大或过小都越难准确贴合验证集数据。

他们之间误差和d之间的关系图像：

![image-20220127105916354](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127105916354.png)

从这个图中可以看出高偏差和高方差造成结果的不同：

- 高偏差（欠拟合）：训练误差很大，验证误差近似等于训练误差。
- 高方差（过拟合）：训练误差很小，验证误差远大于训练误差。

### 正则化和偏差、方差

偏差、方差与正则化算法有着千丝万缕的关系，彼此之间会互相影响。

![image-20220127113040254](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127113040254.png)

正则化参数过大，会导致对参数的惩罚过大， 所有参数近似等于0，此时假设函数近似等于θ0，导致欠拟合。

正则函数过小，正则化项近似为0，假设函数与预测值过度贴合，导致过拟合。

所以，选择一个合适的正则参数是很必要的。

以下为正则化学习算法的相关公式：

![image-20220127113516550](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127113516550.png)

其中训练、验证、测试集的代价函数只针对去掉正则化项的代价函数进行研究。

下图为确定合适的正则化参数的步骤：

![image-20220127113922752](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127113922752.png)

- 分别设定正则化参数值（从0开始以2倍增长，直至增长至一个较大的值为止）。
- 将设定的正则化参数值代入，最小化代价函数，求解最佳参数代入到验证集的代价函数中。
- 一一比较，最小验证集的代价函数对应的正则化参数即为合适的正则化参数。

所以不难发现正则化参数会影响训练集/验证集的误差大小，二者关系如下图：

![image-20220127115304626](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127115304626.png)

### 学习曲线

学习曲线是检验我们的学习算法是否有效、运行是否正常的工具，它可以判断算法是否具有偏差、方差问题。

建立一个二次假设函数模型，对于训练集来说，我们训练集数量越小，其拟合程度就越好；反之，则越差。

而对于验证集来说，训练集数量越大，有助于预测的更贴近现实。

就会得到下图中的学习曲线：

![image-20220127115825278](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127115825278.png)

那么在高偏差/高方差的情况下，学习曲线又会如何变化呢？

#### 高偏差

我们使用一维模型来拟合数据。当训练集样本数量很少时，其不能很好的拟合（但还是会经过几个训练集数据）；随着训练集样本数量的增加，存在一个值令该模型有一个相比之下最好的拟合；继续增加，该模型还是不能很好的拟合（因为一个直线难以太贴合）。这就导致验证数据集的误差从一开始的很大逐渐减小，最后几乎不变；训练集则是一开始很小，逐渐增大，最后几乎不变。

下图为高偏差的学习曲线：

![image-20220127120222685](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127120222685.png)

高偏差且训练集数量极大时，二者误差很接近，曲线几乎重叠。

根据学习曲线，我们可以看出**高偏差情况下，再多的训练数据也无法消除误差。**

![image-20220127120735029](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127120735029.png)

#### 高方差

这次我们使用一个极高次的模型。当训练集样本数量少时，此时会出现过拟合现象；随着训练集样本数量增加，过拟合将会变得越来越困难，训练集误差有所增加，但误差不会太大。对于验证集数据来说，过拟合会导致很难预测新的数据。

下图为高方差的学习曲线：

![image-20220127121507092](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127121507092.png)

高方差且训练集数量比较大时，二者误差差值很大，但随着训练集数目的增大，二者差值会逐渐减小，验证集误差也在逐渐减小。

根据学习曲线，我们可以看出**高方差情况下，大量的数据可以消除误差。**

![image-20220127121812121](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127121812121.png)

### 选取有效的修正方法与模型

#### 修正方法

以下的这些方法什么情况下对于我们修正学习算法有效呢？

![image-20220127122007432](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127122007432.png)

1. 增加训练集数量：解决高方差。
2. 减小特征量数量：解决高方差。
3. 增加特征量数量：解决高偏差。
4. 增加多项式数量：解决高偏差。
5. 减小正则化参数：解决高偏差。
6. 增大正则化参数：解决高方差。

#### 神经网络结构

神经网络结构十分简单（隐藏单元少）时，容易出现欠拟合的问题，计算量小。

神经网络结构十分复杂时，容易出现过拟合的问题，计算大。

但我们通常采用复杂的神经结构网络，调整正则化参数来解决过拟合。

![image-20220127122921745](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220127122921745.png)

至于隐藏层数的选择，我们通常默认选择一层。如果想要更精准，我们可以计算不同层数的验证集的代价函数，选取其中最小的代价函数对应的层数。

## 第十一章：机器学习系统设计

### 确定执行优先级

我们日常生活中使用邮件交流时，有时会出现一些广告邮件，面对这种情况，我们需要建立一个广告分类器：

![image-20220128092624608](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128092624608.png)

以下为建立的一个广告分类器的模型，用向量表示单词是否出现（1表示出现，0表示未出现）：

![image-20220128092406630](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128092406630.png)

以下为减少分类误差的几个方法：

![image-20220128092256391](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128092256391.png)

### 误差分析

解决学习算法相关的实际问题时，我们常常推荐以下的几个方法：

- 先创建一个简单的算法模型
- 使用学习曲线
- 误差分析

![image-20220128092909546](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128092909546.png)

误差分析：手动检查那些被学习算法错误分类的邮件，观察它们有哪些共同特征，共同特征占比最大的就是我们误差分析的重点。

![image-20220128093336576](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128093336576.png)

误差分析常选择的方法之一：数值计算。

一些文字识别软件会将discount/discounts/discounted/discounting这些相似的词语识别为同一个单词，其好处是减小了特征量个数，坏处是会讲一些形似但意义不同的单词也识别为同一个单词。

那么我们如何决定是否使用这些工具呢？答案是使用数值计算去评估这些方法的误差率，随后选择误差率更小的方法。

![image-20220128093918454](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128093918454.png)

需要注意的是，**我们要在交叉验证集上做误差分析，因为对于新数据的预测准确度要比已有数据的预测重要得多。**

### 不对称性分类的误差评估

一个类出现次数比另一个类多很多的情况，我们称为偏斜类（不对称性分类）。如下图y=0的次数要比y=1的次数多很多。

![image-20220128100501839](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128100501839.png)

这种情况下，多预测y=0会使误差下降。但也导致了一个问题，误差减小不再意味着我们的算法更精准，因为我们多预测y=0也会令误差下降。在遇到这种偏斜类时，我们希望有不同的误差度量值来表示算法的精准程度。

其中一种误差度量值叫做查准率和召回率。

#### 查准率和召回率

首先，所有病人的得病和预测情况可以分为以下四种：真阳性、假阳性、真阴性、假阴性。

![image-20220128101524485](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128101524485.png)

查准率：真阳性人数占所有阳性人数的比例。查准率越高越好，预测病人是否真正患癌越准确。

召回率：预测出患癌能够召回及时治疗的额人数占所有患癌人数的比例。召回率越高越好，让病人能够及时得到治疗。

所以总是判别y=0未必是一个好模型，此时召回率等于0，很可能是预测错误。**一个具有高查准率、高召回率的分类模型是好的分类模型。**

在使用查准率和召回率时，我们常常设其判断很少出现的情况（y=1）。

### 精确度和召回率的权衡

情况一：当我们想尽可能避免出现误诊癌症时，我们提升预测癌症的概率，当一个人患癌概率高达百分之七十时，我们才告知他接受治疗。这时我们可以保证接受治疗的绝大多数确实患有癌症，但可能会遗漏一些真正患癌的患者。导致高精确度低召回率。

情况二：当我们想尽可能不遗漏患癌患者时，我们降低预测癌症的概率，当一个人患癌概率达百分之三十时，我们就告知他接受治疗。这是我们可以保证尽量不遗漏患癌患者，但可能使不具有癌症的人也接受了癌症治疗。导致低精确度高召回率。

根据以上两种情况，我们可以得知精确度和高召回率的关系（具体有很多种，根据学习算法的不同而不同。但一定是一个增大一个减小）：

![image-20220128102501000](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128102501000.png)

那么如何判断以下的哪一个算法好呢？需要引入一个评估度量值。

![image-20220128103224177](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128103224177.png)

如果引入平均值作为评估度量值的话，当精确度和召回率差值极大时，这不是我们想要的模型，但平均值会将其预测为最好的结果。所以我们不采用平均值作为评估度量值，而是使用F值：

![image-20220128103520093](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128103520093.png)
$$
F=2\frac{PR}{P+R}
$$
F值越大，模型越好。

### 机器学习数据

有时我们获得具有以下性质的数据有助于我们建立一个更好的机器学习算法

- 有足够大量相关的数据：

    ![image-20220128104746234](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128104746234.png)

- 具有足够多的参数（减少偏差），具有非常大的训练集（减小方差）：

![image-20220128104841410](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220128104841410.png)

## 第十二章：支持向量机（SVM）

### 优化目标

最小化下列式子，即可得到向量机学习得到的最优参数。

![image-20220129083955460](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129083955460.png)

### 大间距

以下为向量机数学模型：

如果y=1，那么z>=1;

如果y=0，那么z<=-1.

与逻辑回归模型不同的是，我们没有设置0为分界点来判断y的取值，而是隔了一段间距，选取1作为分界点：

![image-20220129084928204](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129084928204.png)

如果C取一个相当大的值会如何？

为了最小化代价函数，我们将与C相乘的那一项赋值0，此时我们只需最小化关于参数θ的函数：

![image-20220129123623315](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129123623315.png)

这种情况下我们会得到一个特殊的决策边界（黑线）：

![image-20220129123905305](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129123905305.png)

做黑线的平行线（蓝线），其中这两条蓝线与黑线的间距表示决策边界与结果的距离，称作间距。

黑线与粉线绿线相比，它的间距是最大的，这就是大间距分类器。

关于向量机算法，我们就需要使用大间距分类器有助于我们解决问题。

### 大间隔分类器的数学原理

#### 向量内积

下图讲解了向量内积的集合含义：向量v在向量u上直角投影，p为投影长度。

![image-20220129124902252](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129124902252.png)

那么就有
$$
u^Tv=p*||u||=v^Tu\\
都是v在u上投影所以相等
$$
将这一含义运用到向量机的决策边界上：

![image-20220129125806790](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129125806790.png)

则有
$$
\theta^Tx^{(i)}=p^{(i)}*||\theta||
$$

#### 大间隔分类器

根据向量内积的定义，我们可以进行替换：

![image-20220129130815237](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129130815237.png)

其中p表示样本投影到参数向量上的距离。

情况一：采用左侧非常小间距的决策边界（绿色线）。根据参数向量和决策边界的正交关系绘制出参数向量（蓝色线），投影得到p。如果我们想得到y=1/-1的结果需要让其乘积大于等于1/小于等于-1，由于p非常小，我们就需要增大参数的值，才能满足条件。但这与我们的需求背道而驰（最小化参数），所以向量机不会选取该决策边界。

情况二：采用右侧大间距的决策边界（绿色线）。根据参数向量和决策边界的正交关系绘制出参数向量（蓝色线），投影得到p。如果我们想得到y=1/-1的结果需要让其乘积大于等于1/小于等于-1，由于p足够大，所以允许参数减小，也可以满足条件。这满足我们的需求，所以向量机会选取该决策边界。

数图结合，不难发现p其实就是间距。

以上，就是我们选取大间距分类器的原因。

### 核函数Ⅰ：Kernal function

对于像下图一样复杂的数据，我们需要建立非线性决策边界来进行预测。预测方法就是引入多项式（多特征）进行判断：

![image-20220129132204068](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129132204068.png)

关键问题在于，如何选取更好的特征？这就引出了我们本节内容——核函数。

给出x，依据已有数据l计算新的特征点：

![image-20220129132839890](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129132839890.png)

其中我们设多项式f（特征）表示x和已有数据l（标记点）的相似度函数（核函数）：

![image-20220129133106951](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129133106951.png)

当x与l越接近，f就越趋近1；当x与l相差越大，f就越趋近0.

以下通过一个例子来更好地理解：

可以看到，当x等于l时，f1=1；x越不贴近l，f1越趋近于0.

![image-20220129133424537](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129133424537.png)

核函数参数`σ^2`对于这种模型也有影响：

![image-20220129133757186](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129133757186.png)

核函数具体如何预测：

给定参数和待预测值x，如果x临近l1，那么f1近似等于1，f2和f3近似等于0.带入预测函数后发现大于等于0，即y预测为1.

![image-20220129134143174](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129134143174.png)

当x接近l1/l2时，预测值都为1，否则预测为0，由此可画出决策边界。

### 核函数Ⅱ

#### 选择标记点

获取合适的标记点有助于我们使用核函数决定非线性问题的决策边界。

选择标记点：

- 数据库中所有已有的x赋给标记点

![image-20220129135144117](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129135144117.png)

#### 核函数

向量机使用核函数的过程中，我们直接使用f作为特征值：

![image-20220129141528873](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129141528873.png)

#### 参数

- C可将其看作正则化参数的倒数，其影响着偏差（bias）和方差（variance）。
- σ^2影响特征值的变化速率、偏差和方差。

![image-20220129141928907](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129141928907.png)

### 使用SVM

使用SVM软件包

![image-20220129173328938](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129173328938.png)

涉及到两点：参数C的选择和核函数的选择。

当n很大（问题规模），m很小时（训练集样本），不选取核函数。

当n很小，m很大时，选取高斯核函数（需要选择参数σ^2）。

![image-20220129173905391](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129173905391.png)

实现高斯核函数：



以下为实现高斯核函数的代码，x1、x2分别需要我们输入待预测变量和标记点。

![](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129174653418.png)

需要注意的是，不同的特征变量具有不同的单位其范围可能有很大的不同，所以在使用之前我们需要进行特征值的范围缩放。

其他的核函数：

- 指数核函数
- 字符串核函数：输入数据是文本字符串
- 卡方核函数
- 直方相交核函数

![image-20220129181409012](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129181409012.png)

向量机处理多分类问题：

![image-20220129182034483](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129182034483.png)

logistic regression vs. SVM

- n（问题维数，即特征量数量）相比m（训练集数量）很大：使用logistic regression或者不带核函数的向量机。
- n小，m适中：使用带高斯核函数的向量机。
- n小，m大：创建更多的特征值然后使用logistic regression或者不带核函数的向量机。

根据以上的情况总结可以发现，其实logistic regression和不带核函数的向量机作用很相似，甚至它们二者之间有很多步骤也是相同的。

![image-20220129182904786](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220129182904786.png)

神经网络与这两种算法相比所花时间要更长，因为我们要花时间去锻炼培养一个好的神经网络。

SVM优化是一种凸优化问题，**好的SVM优化软件包总能找到问题的全局最小值。**

## 第十三章：无监督学习——聚类算法

从第十三章开始我们进入无监督学习的内容。

### 无监督学习

监督学习：给定一些标签，用假设函数去拟合它。

![image-20220130082840194](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130082840194.png)

无监督学习：输入一些数据，让算法识别其隐藏的数据结构。

![image-20220130083008762](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130083008762.png)

聚类算法：将上述数据按聚集合并成“蔟”的算法。

![image-20220130083158976](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130083158976.png)

聚类算法的应用：

![image-20220130083400479](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130083400479.png)

### K-Means算法

- 蔟分配：画出两个聚类中心，判断绿点是与红色聚类中心更近还是与蓝色聚类中心更近，以此来把绿点分配给两个聚类中心之一。

![image-20220130083601476](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130083601476.png)

从算法上来讲，就是遍历每个绿点然后将其着红色或蓝色。

![image-20220130083822626](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130083822626.png)

- 移动聚类中心：将两个聚类中心移到它们各自颜色的平均值处。

![image-20220130083945454](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130083945454.png)

然后根据这些数据是与红色聚类中心更近还是与蓝色聚类中心更近，来分配给两个聚类中心之一。

![image-20220130084110318](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130084110318.png)

随后再进行移动聚类中心、蔟分类、移动聚类中心……

最后变成下图，此时再如何迭代图像也不会再改，K均值已经聚合了。

![image-20220130084246940](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130084246940.png)

K-means算法：

输入：

- K（蔟的数量）
- 无标签的训练集数据

![image-20220130084407493](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130084407493.png)

算法执行：

- 随机初始化K蔟聚类中心μ。
- 遍历所有数据，将其着色为距离最近的聚类中心。
- 遍历所有聚类中心进行移动，求取平均值并更新。
- 重复2.3步

![image-20220130084807935](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130084807935.png)

对于分类不明显的算法：

![image-20220130094456351](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130094456351.png)

### 优化目标

优化目标：

- c(i)表示当前数据属于哪一簇中心。
- μk表示第k个蔟中心。
- μc(i)表示c(i)的簇中心。
- 优化目标：计算数据与簇中心的距离，选取出最小的距离进行分簇。（失真代价函数）

![image-20220130094849228](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130094849228.png)

第一步：最小化代价函数求出数据最接近的聚类中心。

第二步：求取簇中心的平均值。

总而言之，K算法做的就是最小化J关于变量c再最小化J关于变量u，然后保持迭代。

![image-20220130095458931](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130095458931.png)

### 随机初始化

K-means 算法中有一个关键步骤，对K算法的簇中心进行随机初始化。接下来我们介绍一下随机初始化的方法。

![image-20220130100021991](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130100021991.png)

随机初始化：

- K应小于样本数量。

- 随机挑选训练集实例作为簇中心。

由于我们随机挑选簇中心，所以可能导致不同的分簇结果。

![image-20220130100406240](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130100406240.png)

局部最小值：

下图中的最后两种情况就是K算法落入了局部最小值，不能很好的最小化畸变函数J，会导致分簇结果变差。

![image-20220130100715779](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130100715779.png)



解决方法就是多次随机初始化K算法，多次运行K算法。然后选取具有最小的代价函数的K算法，也就是畸变值最小的。



![image-20220130101045047](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130101045047.png)

**注意：多次初始化只在分类数K较小（2-10）时起作用，当K很大（几百上千）时往往第一次随机初始化的结果就是很好的结果。**

### 选取聚类数量

肘部法则：

下左图为肘部法则，在K=3时大幅下降，往后再增大K下降就逐渐缓慢。这说明我们最佳的分类数选择为3.

当然还有一些问题不满足肘部法则，如下右图，它一直呈下降趋势。

![image-20220130101644056](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220130101644056.png)

在不同的问题中，我们对分类有不同的要求。所以我们不知如何分类时，可以问问自己，运行K均值聚类的目的是什么？从而来确定K值。

![image-20220131080216544](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131080216544.png)

## 第十四章：降维

### 目的Ⅰ：数据压缩

当出现冗余的特征量/可以整合的数据，我们可以将其进行合并或取出，以实现降维的效果，使得内存存储大大减少：

![image-20220131080838083](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131080838083.png)

三维缩小至二维：

![image-20220131081204591](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131081204591.png)

数据的压缩有助于算法效率的提升。

### 目标Ⅱ：可视化

可视化有助于我们可以很好的了解我们数据发生的变化、特征等等。

原本用五十维来描述的问题。

我们将五十维减小到二维。

![image-20220131082341590](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131082341590.png)

![image-20220131082431856](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131082431856.png)

横轴为GDP，竖轴为人均GDP。

![image-20220131081909706](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131081909706.png)

### 主成分分析（PCA）问题规划1

PCA做的是**找到一个低维平面（图中为一维直线），然后将数据投影到上面使其投影误差（蓝线）长度平方最小。**

![image-20220131083436403](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131083436403.png)

如果选取粉色线作为投影平面，会发现投影误差及其大，这就是PCA选择红线而不是粉线的原因。

![image-20220131083736210](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131083736210.png)

PCA所做的是，如果想将二维降至一维，那么需要寻找一个向量，其方向是使数据投影后最小化投影误差的方向。

普遍来说，**如果想降至K维，那么需要寻找K个向量，其方向是使数据投影后最小化投影误差的方向，将数据投影到这K个向量展开的线性子空间。**

PCA不是线性回归。

线性回归需要注意x和y的关系，而PCA中没有y，所有x都一致看待。

左图为线性回归，右图为PCA。

![image-20220131093333295](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131093333295.png)

### 主成分分析问题规划2

这一节中我们主要介绍主成分分析算法。

以下为主成分分析算法步骤：

- 数据处理

执行均值标准化/特征缩放。	

![image-20220131093914869](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131093914869.png)

- 数据缩放

求出构成低维空间的所有向量u以及表示低维空间的向量z。

![image-20220131094429934](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131094429934.png)

计算方法：

（左侧Σ为协方差矩阵，右侧Σ为求和符号。）`svd`函数是用于求解矩阵U、S和V的协方差矩阵Σ，Σ将会是个n*n矩阵。其中U就是输出的协方差矩阵。

![image-20220131095408953](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131095408953.png)



 首先降至k维，即只保留U的前k个向量。

向量Z的求法就是U的转置乘训练集数据矩阵X。

![image-20220131101121001](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131101121001.png)

PCA算法小结：

![image-20220131101344351](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131101344351.png)

**PCA算法的作用就是让投影误差最小。**

### 主成分数量选择

这一节，我们来了解如何选择PCA算法中的参数k（低维维数）。

![image-20220131102344306](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131102344306.png)



左侧逐一尝试k值，直至下式小于等于0.01；右侧是更简便的方法，利用矩阵S，对于给定的参数k，若下式小于等于0.01，即满足要求，这样只需要执行一次`svd`即可。需要注意的是，需要挑选满足下式的k的最小值。

![image-20220131102733740](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131102733740.png)

![image-20220131103154649](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131103154649.png)

### 压缩重现

考虑到内存和算法效率问题，我们使用压缩，但当我们需要详细复杂的数据、恢复至原来的维度时，如何恢复呢？这一节，我们来解决这个问题。

其实就是对压缩过程求逆运算。

![image-20220131103854389](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220131103854389.png)

压缩过程：
$$
z=U^T_{reduce}x
$$
压缩逆过程：
$$
x_{approx}=U^T_{reduce}z
$$
其中z表示压缩后的数据，x表示压缩前的数据。这一过程，我们称为**压缩数据的重构，本质就是通过压缩表示重建原来的数据x。**

### 应用PCA的建议

使用PCA对监督学习算法进行加速

计算机处理一个长为100像素，宽为100像素的图片，将每一像素作为特征量则问题高达10000维度，此时我们可以使用PCA算法进行降维，对问题实行简化处理。

![image-20220201105312388](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201105312388.png)

注意建立x到z映射应该将PCA算法应用至训练集上。

![image-20220201111134854](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201111134854.png)

PCA的应用以及k的选择：

![image-20220201111426724](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201111426724.png)

**错误用法1：尝试使用PCA去防止过拟合。**使用正则化更好更有效。

![image-20220201111647839](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201111647839.png)

**错误用法2：未尝试原有数据而直接使用PCA。**建议先使用原有数据x，如果这导致占取内存/效率太低再考虑PCA。

![image-20220201111853335](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201111853335.png)

## 第十五章：异常检测问题

这章我们面对的是学习算法中常常遇到的问题——异常检测，它虽然常常用到无监督学习算法中，但其与监督学习算法很类似。

### 问题动机

这一节我们来介绍一下何为异常检测。

下图为飞行器引擎相关的数据：

可以发现在已给数据中，贴近已有数据的指标正常，远离已给数据的指标异常。因为它和我们见过的正常的不一样。这是异常检测的定义之一。

![image-20220201112906398](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201112906398.png)

下面介绍异常检测的另一个说法：

将已有数据进行划分，越靠近中心概率越大，越远离概率越小。

设置一个异常概率，一旦测试集数据的概率小于异常概率，则该数据异常；反之则正常。

![image-20220201113400567](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201113400567.png)

另一种定义，检测已有的特征值数据，进行概率判断。

![image-20220201114050052](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201114050052.png)

### 高斯分布

高斯分布也称为正态分布。

![image-20220201114705264](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201114705264.png)	

μ决定图像中心，σ决定图像的高矮胖瘦。

![image-20220201114826801](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201114826801.png)

参数估计：对正态分布中的平均值、标准差进行计算。

![image-20220201115131744](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201115131744.png)

### 异常检测算法

根据已有训练集的数据，认为它们服从正态分布，将其能取到对应值的概率进行连乘，作为该训练集概率。 

![image-20220201115503585](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201115503585.png)

算法步骤：

1. 选择可以代表异常的特征量。
2. 参数估计，计算平均值和方差。
3. 连乘所有已有数据概率，计算训练集概率。如果小于异常概率，则检测出异常。

![image-20220201115756833](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201115756833.png)

实例：

![image-20220201120505248](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201120505248.png)

### 开发和评估异常检测系统

实数评估：决定保哪些特征以及如何处理这些特征。

假设我们有一些表示异常和正常的标签数据（1表示异常，0表示正常）。

训练集的数据中，我们假设都是正常数据。

验证集和测试集中则带有异常/正常的区分。

![image-20220201121442038](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201121442038.png)

接下来介绍一个引例，来学习如何开发异常检测系统：

飞行器引擎中由10000个正常的引擎，20个异常的引擎。

我们分配6000个正常的引擎给训练集；2000个正常引擎、10个异常引擎分配给验证集；2000个正常引擎、10个异常引擎分配给测试集。

正常引擎以6:2:2分配至训练集、验证集、测试集，异常引擎以1:1分配给验证集、测试集。

![image-20220201123128825](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201123128825.png)

然后我们建立训练集概率`p(x)`。在验证集/测试集中加入预测异常变量y。

对验证集和测试集进行评估标准：

- 真阳性、假阳性、假阴性、真阴性
- 精确率/准确度
- F score 



也可以使用验证集去确定误差参数。

![image-20220201124232689](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201124232689.png)

### 异常检测vs监督学习

异常检测应用场景：

- 正常/异常样本很少。
- 很多种异常类型的样本，难以被学习算法学习。
- 未知的异常类型样本。

![image-20220201130340637](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201130340637.png)

监督学习：

- 大量正常、异常样本
- 足够多的样本量，能够被预测得到。



二者应用的实例：

![image-20220201134128972](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201134128972.png)

### 选择特征

异常检测算法需要应用到服从高斯分布的变量（如下图图1）。

但往往有些变量不服从高斯分布（如下图图2），这时我们需要对这些变量进行调整。

比如用对数运算来代替该变量（log(x+c)代替x，其中c为常数）、用指数运算代替该变量（x^1/3代替x）等等。

这些参数都任意可调整，只要调整后变量服从高斯分布即可（如下图图3）。

![image-20220201134408436](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201134408436.png)

如何得到异常检测算法的特征？

我们想要正常数据对应的概率较高；异常数据对应的概率较低。

但很容易发生的错误是，异常数据和正常数据概率相似，甚至前者大于后者。

这时我们需要找到一个新特征，在新特征的图上进行观察，是否能对正常数据和异常数据对应概率进行区分。

所以这个过程实际就是**看看哪里出了错（没能很好地区分正常数据与异常数据），看看这里能不能启发你建立一个新的很好的特征量，能够帮助将异常数据与正常数据进行区分开来。**

![image-20220201134956522](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201134956522.png)

还有一种方法就是选择**在异常时容易很大或很小的特征值**作为区分。

![image-20220201135555119](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201135555119.png)

### 多变量高斯分布

CPU加载越多，内存使用越大。左上角的绿叉表示CPU加载少但内存使用小，明显出现异常。

异常检测算法却将其落入了正常的概率之内（右图），即判断它属于正常情况。

为了解决这种问题，我们需要改良异常检测算法，为此引入多变量高斯分布。

![image-20220201140009559](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201140009559.png)

以下为多变量高斯分布步骤：

首先建立`p(x)`作为一整个模型，需要计算平均值和协方差作为参数。

然后计算下式：

![image-20220201140418006](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201140418006.png)

`p(x)`模型：

下图中高度表示`p(x)`。协方差的改变导致x1,x2方差的变化，引起图形高矮胖瘦的改变。

![image-20220201140809707](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201140809707.png)

两个变量的方差也可以不同：

![image-20220201140957695](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201140957695.png)

多元高斯分布还有一个好处是，可以观察建立两个变量的相关性：

![image-20220201141209281](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201141209281.png)

协方差矩阵副对角线控制相关性，都为正两个变量正相关，都为负两个变量负相关。

平均值则控制高斯分布聚集的中心：

![image-20220201141355426](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201141355426.png)

### 使用多变量高斯分布的异常检测

多元高斯分布及其参数估计问题：

![image-20220201141530190](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201141530190.png)

如何将多元高斯分布应用到异常检测算法中：

- 计算平均值、协方差
- 计算新数据的多元高斯分布概率，小于异常概率则异常。

![image-20220201141814831](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201141814831.png)

多元高斯分布和原模型（高斯分布）的区别：

原模型的图像总是轴对称的：

![image-20220201142046090](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201142046090.png)

而多元高斯分布则可以倾斜方向。

其实，**原模型就是特殊的多元高斯分布。**

将多元高斯分布中的协方差矩阵的主对角线改为各变量的方差，副对角线改为零即可。

![image-20220201142222972](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201142222972.png)

那么什么时候使用原模型，什么时候使用多元高斯分布呢？

原模型：

- 大部分情况都使用原模型，但在特殊情况下，需要手动建立一个新特征量才能解决问题。
- 计算量更小、效率更高。
- 即使m（训练集数量）很小也可以使用。

多元高斯分布：

- 即使在特殊情况下（特征值关系不服从对称/不相关的情况下），也可以很好的解决问题。
- 计算量大、效率低。
- 必须m远大于n（十倍及以上）或者协方差矩阵可逆才可使用。（如果不可逆有可能是特征量过度冗余。）

![image-20220201142853415](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220201142853415.png)

总结：异常检测算法就是自动地捕捉正样本和负样本、各种特征值间的关系，如果它发现某些特征的组合值是异常的，它会将其标为异常样本。

## 第十六章：推荐系统

### 问题规划

推荐系统的作用就是根据用户的喜好推荐给他他可能喜爱的物品。

下面为用户给电影评分的例子：

![image-20220202095304678](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202095304678.png)

推荐系统在其中的作用是根据已有的用户评分去推测用户尚未观看的电影可能会给的评分，根据评分来判断是否要推送给用户。

### 基于内容的推荐算法

继续引用上一节的电影评分的例子，来介绍一下基于内容的推荐算法的操作。

引入两个特征量：爱情相关度和动作相关度，构成一个特征向量。

我们可以将预测用户对某一电影的评分这一问题看作线性回归的问题。我们要学习一个三维参数向量，然后我们要预测用户对某电影的评分。方法就是参数向量的转置与特征向量的内积。

![image-20220202095902105](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202095902105.png)

问题整合：

![image-20220202100835454](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202100835454.png)

通过最小化代价函数来学习参数。

![image-20220202101002868](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202101002868.png)

梯度下降：

![image-20220202101125599](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202101125599.png)

### 协同过滤

特征学习：

有时候我们很难提前知道特征值的具体值，我们想让学习算法自己预测特征值。

下例中我们不知道一部电影的爱情相关度和行动相关度，在已知用户对爱情电影和行动电影的喜爱程度的情况下，我们可以反推特征值。

![image-20220202101443547](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202101443547.png)

求解特征值的具体办法：已知参数最小化代价函数求取特征值。

![image-20220202102320037](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202102320037.png)

**结合计算参数和计算特征值两个算法，循环往复，使得预测值越来越精确，这就是协同过滤。**

![image-20220202102432616](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202102432616.png)

但是用这种方法来进行协同过滤，需要该用户对很多电影评过分、预测的电影被很多用户评过分，有很多限制，下一节我们讲解协同过滤的改进。

协同过滤就是当我们执行算法时，要观察大量用户观察这些用户的实际行为来协同地得到更佳的预测结果，这需要大量用户来进行反馈评价使算法优化进一步去预测其他用户的反馈评价。所以协同的另一层含义是指每位用户都在帮助算法更好地进行特征学习。

### 协同过滤算法改进

将两个最小化代价函数合并起来，即可得到两个参数的代价函数，同时对它们求最小值，即可获得最佳的方案。这就是协同过滤算法的改进。（不用加入θ0和x0）

![image-20220202103801392](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202103801392.png)

协同过滤算法：

- 用小得随机值初始化特征值、参数值。
- 使用梯度下降最小化代价函数求得特征值、参数值。
- 计算`θ^Tx`预测。

![image-20220202104136120](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202104136120.png)

使用协同过滤算法，我们就可以同时判断电影的成分相关度、用户的喜好以及对电影评测分数的预测。

### 矢量化：低秩矩阵分解

将不同用户对不同电影的评分写入矩阵Y中：

![image-20220202104525795](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202104525795.png)

低秩矩阵分解：

![image-20220202111459304](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202111459304.png)

寻找相关电影：

![image-20220202111734238](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202111734238.png)

### 均值规范化

之前的算法我们都是针对已有评分进行推荐，此时加入一个新用户Eve，她没有对任何电影进行评分。这种情况下我们如何给这位用户推荐电影呢？这就涉及到了均值规范化的内容。

将其加入矩阵：

![image-20220202112245166](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202112245166.png)

计算每一个电影的平均得分，写入矩阵中。

计算的预测值再加上平均值。

![image-20220202112717136](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220202112717136.png)

此处，归一化体现在将矩阵Y的每一行的和都为0.

如果有某一电影没有评分，可以使用该算法的类似方法，归一化每一列。

## 第十七章：大规模学习算法

### 学习大数据集

得到越多数据，精确度越高。

![image-20220203090835848](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203090835848.png)

但需要注意的是，得到一个大数据时，对它直接进行梯度下降计算是十分复杂的。所以我们先选出一部分进行测试。

如果模型是个高方差模型，增加数据量有助于误差的减少，但如果模型是个高偏差模型，增加数据量则无用。

![image-20220203091113678](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203091113678.png)

接下来的几节我们来讲一下选取什么方法来处理大数据。

### 随机梯度下降

求取出代价函数后，我们需要使用梯度下降求取最优解，但对于大数据使用梯度下降效率太慢、计算太多，所以我们对梯度下降进行改进——随机梯度下降。

线性回归中的梯度下降需要我们遍历所有数据，然后计算其偏导。当数据量很大时，效率极低。

![image-20220203091737930](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203091737930.png)

随机梯度下降：

1. 随机打乱数据集。（数据预处理）
2. 遍历训练样本以寻找最优参数。

![image-20220203092421573](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203092421573.png)

它与批量梯度下降的区别在于，它每一次只考虑一个训练样本，逐渐地向最优参数逼近。

随机梯度下降会在最优参数附近徘徊，批量梯度下降会得到最优参数。在实际应用中，随机梯度下降得到的近似最优参数已满足需求。

外循环次数一般根据数据量而定，在一次到十次不等。

![image-20220203092820828](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203092820828.png)

### Mini-Batch 梯度下降

Mini-Batch 梯度下降是一种基于批量梯度下降和随机梯度下降之间的梯度下降方法。每次迭代，它选取b个样本量(b的范围在2-100之间，一般选取10)。有时它处理大数据的速度要快于随机梯度下降。

![image-20220203093259655](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203093259655.png)



Mini-Batch 梯度下降要快于批量梯度下降，因为它一次处理b个数据就可以更新参数，总共只需要执行(m/b)次迭代。

![image-20220203093505498](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203093505498.png)

Mini-Batch 梯度下降与随机梯度下降相比，只要找到一个合适的向量化方式，就会快很多。

### 随机梯度下降收敛

检查收敛：

随机梯度下降检查收敛：

- 计算代价函数。
- 在学习期间，在更新参数之前计算代价函数。
- 每一千次迭代，计算代价函数的平均值，画图进行观察。

![image-20220203094157549](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203094157549.png)

上行两图随着迭代次数增加代价函数减小，说明算法收敛。如果从一千次迭代计算平均值变成五千次迭代计算平均值，曲线会变得更加平滑。（随机梯度下降曲线陡峭是因为它总是在最优值范围徘徊。）

下行图一，如果曲线平缓，可以尝试从一千次迭代计算平均值变成五千次迭代计算平均值，可能会发现代价函数的下降，即算法有效；若代价函数仍然平缓，则说明算法无效，没有在进行学习。

下行图二，随着迭代次数增加代价函数增大，说明算法发散，这时减小学习率α可能会有所改善。

![image-20220203094600055](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203094600055.png)

学习率常常被设为一个固定常数。缓慢减小学习率，可以使随机梯度下降获得更好的参数。

![image-20220203095227239](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203095227239.png)

### 在线学习

在线学习适用于建立一个网站，有大量的用户进行操作（自主输入数据，已输入的数据学习过后不会保存），使得不用保存数据集，每时我们都可以获得免费的数据。

在线学习结合logistic regression算法（蓝字）：

![image-20220203095747272](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203095747272.png)

优点在于可以适应用户变喜好的变化。

以下为在线学习另一个适用的例子——产品搜索。

用户访问出售手机网站，我们一次性推送十个手机给用户，问题在于如何在100个手机中选取10个推送给用户（符合用户的搜索要求）。

它建立特征量x用于记录关于手机的信息，y则标记用户是否点入链接。

在线学习算法不断结合该用户的是否点击以及要求的手机信息来决定给用户推送的内容。而不是采取已有的数据量。

![image-20220203100340690](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203100340690.png)

其他在线学习例子：

![image-20220203100458372](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203100458372.png)

**在线学习——机器一直在学习，它不保存数据集，而是通过所有用户在线反馈来获取数据。**

### 减少映射和数据并行：Map reduce

处理大数据往往一个计算机是不够的，需要多个计算机联合合作。这一节介绍的内容相比随机梯度下降可以处理规模更大的问题。

Map reduce 核心思想：

将所有数据分成n份（根据有几个计算机解决问题而定），每一份数据分配给每一个计算机，这样每个计算机只用处理1/n份原来的数据，使得计算效率提升。每份数据计算完成后，发送至中心服务器整合所有结果

![image-20220203101810339](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203101810339.png)

其与批量梯度下降所得结果相同：

![image-20220203101830647](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203101830647.png)

Map reduce 思想示意图：

![image-20220203101941373](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203101941373.png)

问题关键在于如何对训练集数据进行整合：只要学习算法可以被表示为函数的求和形式即可使用Map reduce.

![image-20220203102148385](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203102148385.png)

Map reduce 不仅可以应用于多个计算机之间，也可以运用在具有多核的单个计算机中。

![image-20220203102351274](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220203102351274.png)

**Map reduce 的关键就是允许我们并行化计算机器学习问题。**

## 第十八章：应用实例——照片OCR

###  问题描述和OCR流水线

照片OCR全称为**照片光学字符识别。**

它的实现步骤是：

- 文字识别：给定某张照片，它将图片扫描一遍，然后找出照片中的文字信息。
- 文字分割：重点关注这些文字信息，对区域中的文字的字符进行逐一分割。
- 字符分类：字符逐一显示并记录出来。

![image-20220204092128307](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204092128307.png)

 照片OCR流水线：

![image-20220204092352183](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204092352183.png)

### 滑动窗口

这一节我们来介绍如何使用滑动窗口实现对图片成分的检测。

引用上一节的例子，我们先对行人进行检测。

![image-20220204092856676](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204092856676.png)

首先让学习算法学习哪些是行人。

![image-20220204092843567](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204092843567.png)

然后使用一个固定大小的矩形不断滑动来扫描整个图像，扫描过程中就判断了当前是否图像是否是行人（分类判断）。

随后不断增大扫描矩形，重复上述操作。

![image-20220204093042602](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204093042602.png)

接下来我们回到文字扫描的例子，分析OCR的工作流程原理。

与行人检测类似，首先让机器学习哪些是文字，哪些不是文字。

![image-20220204093330674](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204093330674.png)

首先，OCR使用滑动窗口分类器进行分类，是文字部分的则标记为白色矩形（下行左图），随后对所有白色矩形进行扫描，若附近范围内有白色矩形则进行合并。合并后检查合并后的长宽比（一般文字的字高不会大于字宽），若长宽比异常，则舍弃文本内容。

![image-20220204093609133](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204093609133.png)

接下来OCR进行字符分割。

首先依旧是让机器学习哪一些该分割，哪一些不该分割的部分。

随后使用滑动窗口分类器遍历整个图像，对于该分割的部分进行分割。

![image-20220204094028250](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204094028250.png)

字符识别则是前面监督学习算法中神经网络的内容。

### 获取大量数据和人工数据

前面学习监督学习算法的过程中可以发现，使用大量训练数据令机器进行学习对于我们的算法大有裨益。但问题在于如何获得大量数据呢？

机器学习中有一个获得数据的方法——人工数据合成，它并不适用于所有问题，而且将其运用到特定问题时经常需要思考改进并且深入了解它。但如果将其运用到你的机器学习问题，有时它可以为你的学习算法获得大量数据。

人工数据合成有两种形式：

- 自己从零开始创造新数据。
- 我们已有小的训练集，以某种方式扩充训练集。

接下来，我们用字符识别这个例子来讲解人工数据合成的两种形式。

左侧为我们要识别的字符，右侧是我们在网上下载的免费字体库与背景进行随机合成的人工数据。

随后我们让机器进行学习即可对左侧字符进行识别。

这一种是自己从零开始创造新数据。

![image-20220204095231423](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204095231423.png)

左侧为我们已有的训练集，随后我们对已有训练集进行拉伸变换，生成16个新的大的训练集。

这就有助于我们获得新的人工数据。这种方法基于我们对不同数据的不同失真方法。（例如：字体进行拉伸缩小，声音加入不同的背景音。）

![image-20220204095445700](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204095445700.png)

一定要针对我们分析的数据使用不同的、有代表性的失真方法，这样生成的新的训练集才是有效的。随机使用不同的失真方法对于训练集数据无益。

![image-20220204095923155](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204095923155.png)

获得更多数据的讨论：

- 确保我们当前的算法获得更多的数据性能可以变得更好，而不是没有变化（高偏差）。
- 思考获得10倍以上的数据需要花费多长时间？

![image-20220204100402993](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204100402993.png)

### 上限分析：下一步工作的流水线

**上限分析有助于指导我们的工作流中哪一部分最值得花时间研究。**

继续使用OCR的例子：

OCR分为以下几大模块，那么以下哪一模块最值得我们花费时间研究呢？

上限分析做的是以下内容：

- 建立一个模块算法的评价量度指标。

- 对某一模块进行分析。
- 人为的判断出哪些是文字，告知机器。（遍历测试集，直接给出正确答案。）得到其评价量度指标的具体值。
- 针对其他模块也进行上述操作。
- 分析所有评价量度指标的可增长空间，这就代表着如果性能接近完美，系统性能增益多少。

![image-20220204101806685](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204101806685.png)

下面为面部识别的流水线：

增益最大的是我们最值得研究的部分。

![image-20220204102036790](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204102036790.png)

**做好上限分析，知道我们应该把重点放在哪一模块对我们大有用处。**

## 第十九章：总结

![image-20220204102455556](C:\Users\dingdang\AppData\Roaming\Typora\typora-user-images\image-20220204102455556.png)

